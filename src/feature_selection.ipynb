{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve, make_scorer, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy.ma.core import sqrt\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n",
    "from datetime import datetime\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer\n",
    "# import mrmr\n",
    "# from mrmr import mrmr_classif\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD_Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWCASE = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/Data_Dictionary_Showcase.csv'\n",
    "CODING = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/cohort_ad_update_coding_participant_i0_a0.csv'\n",
    "AD_Death_70 = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/feature_selection/01082024_data_ad_chunk58_ad_death_70_merged.csv'\n",
    "AD_Death_70_clear = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_merged_clear.csv'\n",
    "AD_Death_70_drop = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_merged_clear_drop.csv'\n",
    "AD_Death_80_orig = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/train/ad_death_data_decoded_80.csv'\n",
    "AD_Death_80_70_columns = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/01082024_data_ad_chunk58_ad_death_80_70_merged_column_record.tsv'\n",
    "AD_Death_80_70 = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/01082024_data_ad_chunk58_ad_death_80_70_merged.tsv'\n",
    "AD_Death_70_numeric = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_numeric.tsv'\n",
    "AD_Death_70_norm = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_norm.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCI_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWCASE = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/Data_Dictionary_Showcase.csv'\n",
    "CODING = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/cohort_ad_update_coding_participant_i0_a0.csv'\n",
    "MCI_AD_70 = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/feature_selection/01082024_data_ad_chunk58_mci_ad_70_merged.csv'\n",
    "MCI_AD_70_clear = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_merged_clear.csv'\n",
    "MCI_AD_70_drop = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_merged_clear_drop.csv'\n",
    "MCI_AD_80_orig = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/train/mci_ad_data_decoded_80.csv'\n",
    "MCI_AD_80_70_columns = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged_column_record.tsv'\n",
    "MCI_AD_80_70 = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged.tsv'\n",
    "MCI_AD_70_numeric = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_numeric.tsv'\n",
    "MCI_AD_70_norm = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_norm.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## function_column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(df):\n",
    "    for col in df.columns:\n",
    "        # if nan exists\n",
    "        if df[col].isnull().values.any():\n",
    "            print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_difference(df1, df2, path= '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/delete_list.txt'): # path = 'data/weimin/01282024/delete_list.txt'\n",
    "    df1_cols = set(df1.columns)\n",
    "    df2_cols = set(df2.columns)\n",
    "    delete_list = list(df1_cols.difference(df2_cols))\n",
    "    with open(path, 'w') as f:\n",
    "        for item in delete_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    return delete_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column in df1 to df2\n",
    "def column_add(df1, df2, col):\n",
    "    df2_copy = df2.copy()\n",
    "    df2_copy[col] = df1[col]\n",
    "    return df2_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the columns in df that the column name has substring in delete_list\n",
    "def column_delete(df, delete_list):\n",
    "    df_copy = df.copy()\n",
    "    for col in delete_list:\n",
    "        print(f'column name is {col}')\n",
    "        print(\"*\"*20)\n",
    "        print(df_copy.filter(like=col).columns)\n",
    "        # save print to file\n",
    "        with open('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/delete_list.txt', 'a') as f:\n",
    "            f.write(f'column name is {col}\\n')\n",
    "            f.write(str(df_copy.filter(like=col).columns))\n",
    "            f.write(\"\\n\")\n",
    "        df_copy = df_copy.drop(df_copy.filter(like=col).columns, axis=1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_merge(df1, df2, id):\n",
    "    df1_copy = df1.copy()\n",
    "    df2_copy = df2.copy()\n",
    "    df1_copy[id] = df1_copy[id].astype(int)\n",
    "    df2_copy[id] = df2_copy[id].astype(int)\n",
    "    df_merged = pd.merge(df1_copy, df2_copy, on=id, suffixes=('_df1', '_df2'))\n",
    "    \n",
    "    columns_to_drop = [col for col in df_merged.columns if col.endswith('_df2')]\n",
    "    df_merged.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    df_merged.columns = [col.replace('_df1', '') for col in df_merged.columns]\n",
    "    \n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the columns in df that the column name has substring in delete_list\n",
    "def column_substring(df, substring_list):\n",
    "    df_copy = df.copy()\n",
    "    new_col_list = []\n",
    "    for col in df.columns:\n",
    "        org_name = col.split('_')[0]\n",
    "        if org_name in substring_list:\n",
    "            new_col_list.append(col)\n",
    "    df_copy = df_copy[new_col_list]\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce a dataframe records the columns of df\n",
    "include the following information:\n",
    "1. column name\n",
    "2. column type (numeric, date, categorical, multiple_value)\n",
    "3. column value_categorical\n",
    "4. column value_multiple\n",
    "5. availability\n",
    "6. variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_type(column):\n",
    "    column_copy = column.copy()\n",
    "    column_copy = column_copy.dropna()\n",
    "\n",
    "    def is_date(value):\n",
    "        try:\n",
    "            datetime.strptime(str(value), '%d/%m/%Y')\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        evaluated = column_copy.apply(eval)\n",
    "        if any(isinstance(item, (list, np.ndarray)) for item in evaluated):\n",
    "            return 'multiple'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        numeric = pd.to_numeric(column_copy, errors='raise')\n",
    "        if pd.notna(numeric).all():\n",
    "            return 'numeric'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if column_copy.apply(is_date).all():\n",
    "        return 'date'\n",
    "    \n",
    "    # if column_copy name is 'Date of all cause dementia report'\n",
    "    if 'Date' in column_copy.name:\n",
    "        # change 'Date is unknown' to nan\n",
    "        # column_copy = column_copy.replace('Date is unknown', np.nan)\n",
    "        if column_copy.apply(is_date).any():\n",
    "            return 'date'\n",
    "    \n",
    "    return 'categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_values(df, df_columns):\n",
    "    df_copy = df.copy()\n",
    "    df_columns_copy = df_columns.copy()\n",
    "    value_list = ['Location could not be mapped', 'Date uncertain or unknown', 'Time uncertain/unknown', 'Preferred not to answer', 'Do not know', 'Prefer not to answer']\n",
    "    change_value = {'Less than one':0.5, 'Less than an hour a day':0.5, 'Less than a year':0.5, 'Less than once a year':0.5, 'Never had sex': -1}\n",
    "    for idx, row in df_columns_copy.iterrows():\n",
    "        col_name = row['name']\n",
    "        for val in value_list:\n",
    "            if val in df_copy[col_name].unique():\n",
    "                df_copy[col_name] = df_copy[col_name].replace(val, np.nan)\n",
    "        for key, value in change_value.items():\n",
    "            if key in df_copy[col_name].unique():\n",
    "                df_copy[col_name] = df_copy[col_name].replace(key, value)\n",
    "        try:\n",
    "            numeric = pd.to_numeric(df_copy[col_name], errors='raise')\n",
    "            numeric = numeric.dropna()\n",
    "            if pd.notna(numeric).all():\n",
    "                df_columns_copy.loc[idx, 'type'] = 'numeric'\n",
    "        except:\n",
    "            pass\n",
    "    return df_copy, df_columns_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_record(df):\n",
    "    df_copy = df.copy()\n",
    "    # create df_columns inluding column name, data type, number of unique values, number of missing values\n",
    "    df_columns = pd.DataFrame(columns=['name', 'type', 'values', 'value_num', 'availability', 'variance'])\n",
    "    df_columns['name'] = df_copy.columns\n",
    "    for idx, row in df_columns.iterrows():\n",
    "        col_name = row['name']\n",
    "        df_columns.loc[idx, 'type'] = column_type(df_copy[col_name])\n",
    "    return df_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in function column_values:\n",
    "1. change the nan value\n",
    "    1. the multiple_value add '[]' to nan row\n",
    "    2. the categorical add\n",
    "    3. change date type, if df inclue 'Date is unknown', change to nan\n",
    "2. count the different values for multiple_value and categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_values(df, df_columns):\n",
    "    df_copy = df.copy()\n",
    "    df_columns_copy = df_columns.copy()\n",
    "    for idx, row in df_columns_copy.iterrows():\n",
    "        col_name = row['name']\n",
    "        if row['type'] == 'numeric':\n",
    "            df_columns_copy.loc[idx, 'value_num'] = 1\n",
    "        if row['type'] == 'multiple':\n",
    "            # print(col_name)\n",
    "            # change nan to '[]'\n",
    "            df_copy[col_name] = df_copy[col_name].fillna('[]')\n",
    "            df_columns_copy.loc[idx, 'values'] = list(reduce(lambda x, y: x.union(y), df_copy[col_name].apply(eval).apply(set)))\n",
    "            df_columns_copy.loc[idx, 'value_num'] = len(df_columns_copy.loc[idx, 'values'])\n",
    "        if row['type'] == 'categorical':\n",
    "            df_columns_copy.loc[idx, 'values'] = df_copy[col_name].unique()\n",
    "            df_columns_copy.loc[idx, 'value_num'] = len(df_columns_copy.loc[idx, 'values'])\n",
    "        if row['type'] == 'date':\n",
    "            df_copy[col_name] = df_copy[col_name].replace('Date is unknown', np.nan)\n",
    "    return df_copy, df_columns_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " check the number of features, all the features are organized as 'feature names[ | instance 0][ | array 0][_categorical names]', here what we do is to find the original number of the different features:\n",
    "\n",
    " 1. split the column name by '_'\n",
    " 2. find how many different features are exluding 'Participant ID', 'label', 'Date of Death' 'Date of Alzheimer's Disease reported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_features_check(df):\n",
    "    column_list = df.columns\n",
    "    # apply split('_') to each column name\n",
    "    column_list = [col.split('_')[0] for col in column_list]\n",
    "    # find the different column name number\n",
    "    print(f'the different column name number is {len(set(column_list))}')\n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## function_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn simple imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_impute(df, mode='mean'):\n",
    "    df_copy = df.copy()\n",
    "    if mode == 'mean':\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    elif mode == 'median':\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    elif mode == 'most_frequent':\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    elif mode == 'constant':\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "    else:\n",
    "        print('mode error')\n",
    "\n",
    "    imputer.fit(df_copy)\n",
    "    df_imputed = pd.DataFrame(imputer.transform(df_copy), columns=df_copy.columns)\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn iterative imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative imputer\n",
    "def iterative_impute(df):\n",
    "    df_copy = df.copy()\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_copy))\n",
    "    df_imputed.columns = df_copy.columns\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn knn imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn imputer\n",
    "def knn_impute(df):\n",
    "    df_copy = df.copy()\n",
    "    imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_copy))\n",
    "    df_imputed.columns = df_copy.columns\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## function_numeric change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01282024_1:\n",
    "- imputer: \n",
    "    - knn imputer\n",
    "        - n_neighbors=2, weights=\"uniform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_other_to_numeric_all(df, df_columns, idx_columns):\n",
    "\n",
    "    df_numeric = pd.DataFrame()\n",
    "    df_idx = df[idx_columns]\n",
    "    df_feature = df.drop(columns=idx_columns)\n",
    "\n",
    "    # change the event day as the age at the event happen\n",
    "    def age_at_event(event, year=None, month=None, day=None):\n",
    "        # event, year, month, day are all columns\n",
    "        # year, month, day are birth date\n",
    "        # get the name of event\n",
    "        age_at_event = 'Age at ' + event.name\n",
    "        df_copy = pd.DataFrame(columns=['birth_date', age_at_event])\n",
    "        if day is not None and not day.empty:\n",
    "            df_copy['birth_date'] = month.astype(str) + \"/\" + day.astype(str) + \"/\" + year.astype(str) # note: if there exists nan, then change the format to '%d/%m/%Y'\n",
    "        elif month is not None and not month.empty:\n",
    "            if month.dtype == 'object':\n",
    "                # map month to number\n",
    "                month_map = {'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05',\n",
    "                            'June': '06', 'July': '07', 'August': '08', 'September': '09', 'October': '10',\n",
    "                            'November': '11', 'December': '12'}\n",
    "                month = month.map(month_map)\n",
    "            df_copy['birth_date'] = month.astype(str) + \"/01/\" + year.astype(str) # note: if there exists nan, then change the format to '%d/%m/%Y'\n",
    "        else:\n",
    "            df_copy['birth_date'] = \"01/01/\" + year.astype(str)\n",
    "        df_copy['birth_date'] = pd.to_datetime(df_copy['birth_date'], format='%m/%d/%Y', errors='coerce') # note: if there exists nan, then change the format to '%d/%m/%Y'\n",
    "        event = pd.to_datetime(event, format='%m/%d/%Y', errors='coerce') # note: if there exists nan, then change the format to '%d/%m/%Y'\n",
    "        \n",
    "\n",
    "        df_copy[age_at_event] = event - df_copy['birth_date']\n",
    "        df_copy[age_at_event] = df_copy[age_at_event].apply(lambda x: x.days/365)\n",
    "        # round to 1 decimal\n",
    "        df_copy[age_at_event] = df_copy[age_at_event].apply(lambda x: round(x, 2))\n",
    "        # rename \n",
    "        return pd.DataFrame(df_copy[age_at_event])\n",
    "\n",
    "    def multiple_label_data(column, name):\n",
    "        column_copy = column.copy()\n",
    "        column_copy.fillna('[]', inplace=True)\n",
    "        column_copy = column_copy.apply(eval)\n",
    "        # acquire unique values\n",
    "        unique_values = set()\n",
    "        for values_list in column_copy:\n",
    "            unique_values.update(values_list)\n",
    "\n",
    "        dummy_df = pd.DataFrame()\n",
    "        result_df = pd.DataFrame()\n",
    "        # Create new column and encode it as 0 or 1\n",
    "        for value in unique_values:\n",
    "            col_name = name + '_' + str(value)\n",
    "            dummy_df[col_name] = column_copy.apply(lambda x: 1 if value in x else 0)\n",
    "\n",
    "        zero_name = name + '_' + 'zeros_count'\n",
    "        total_columns = len(dummy_df.columns)\n",
    "        zero_percentage_name = name + '_' + 'percentage_of_zeros'\n",
    "\n",
    "        result_df[name] = column.copy()\n",
    "        result_df[zero_name] = (dummy_df == 0).sum(axis=1)\n",
    "        result_df[zero_percentage_name] = (result_df[zero_name] / total_columns) * 100\n",
    "\n",
    "        # delete original column\n",
    "        # ad_death_data_90.drop(columns=['participant.p6152_i0'], inplace=True)\n",
    "\n",
    "        return dummy_df, result_df\n",
    "\n",
    "    def determine_column_type(col_name):\n",
    "        # find col_name in name and return type\n",
    "        return df_columns[df_columns['name'] == col_name]['type'].values[0]\n",
    "\n",
    "    def from_other_to_numeric(col_name):\n",
    "        df_copy = df_feature.copy()\n",
    "        column_type = determine_column_type(col_name)\n",
    "        if column_type == 'numeric':\n",
    "            df_copy_imputed = knn_impute(pd.DataFrame(df_copy[col_name]))\n",
    "            return df_copy_imputed\n",
    "        elif column_type == 'categorical':\n",
    "            return pd.get_dummies(df_copy[col_name], prefix=col_name, dtype=float, dummy_na=False)\n",
    "        elif column_type == 'multiple':\n",
    "            dummy_df, result_df = multiple_label_data(df_copy[col_name], col_name)\n",
    "            return dummy_df\n",
    "        elif column_type == 'date':\n",
    "            # change date to age\n",
    "            return age_at_event(df_copy[col_name], year=df_copy['Year of birth'], month=df_copy['Month of birth'])\n",
    "        else:\n",
    "            print('please rechoose')\n",
    "            return df_copy\n",
    "    \n",
    "    for column in df_feature.columns:\n",
    "        df_numeric = pd.concat([df_numeric, from_other_to_numeric(column)], axis=1)\n",
    "    df_numeric = pd.concat([df_idx, df_numeric], axis=1)\n",
    "    return df_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaling: Also known as feature scaling, it scales the data to a specified range, typically [0, 1]. This method maps the data into the desired interval using a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_minmaxscaler(train, target):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_train = scaler.fit_transform(train)\n",
    "    return pd.DataFrame(scaled_train, columns=train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust Scaling: This method scales data using the median and interquartile range, making it resistant to the influence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_robustscaler(train, target):\n",
    "    scaler = RobustScaler()\n",
    "    scaled_train = scaler.fit_transform(train)\n",
    "    return pd.DataFrame(scaled_train, columns=train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1, l2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_normalizer(train, target, mode='l2'):\n",
    "    scaler = Normalizer(norm=mode)\n",
    "    scaled_train = scaler.fit_transform(train)\n",
    "    return pd.DataFrame(scaled_train, columns=train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zscore: standard normaliztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_standard(train, target):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(train)\n",
    "    return pd.DataFrame(scaled_train, columns=train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yeo-johnson and box-cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_power(train, target, mode='yeo-johnson'):\n",
    "    scaler = PowerTransformer(method=mode)\n",
    "    scaled_train = scaler.fit_transform(train)\n",
    "    return pd.DataFrame(scaled_train, columns=train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function_label generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate label by cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generate(df, day, cutoff, buffer=False, buffer_percent=0.1):\n",
    "    df_copy = df.copy()\n",
    "    # sort by day\n",
    "    df_copy = df_copy.sort_values(by=day, ascending=True)\n",
    "\n",
    "    def delete_buffer(buffer_percent=0.1):\n",
    "        # find cutoff index which is the most close to cutoff\n",
    "        cutoff_index = df_copy[day].sub(cutoff).abs().idxmin()\n",
    "        # delete buffer_percent/2 before and after cutoff\n",
    "        buffer_num = int(len(df) * buffer_percent / 2)\n",
    "        buffer_init = cutoff_index - buffer_num\n",
    "        buffer_end = cutoff_index + buffer_num\n",
    "        print(f'buffer_init is {buffer_init}')\n",
    "        print(f'buffer_end is {buffer_end}')\n",
    "        print(f'cutoff_index is {cutoff_index}')\n",
    "        return buffer_init, buffer_end\n",
    "\n",
    "    if buffer:\n",
    "        buffer_init, buffer_end = delete_buffer(buffer_percent)\n",
    "        # delete buffer\n",
    "        df_copy = df_copy.drop(df_copy.index[buffer_init:buffer_end])\n",
    " \n",
    "    df_copy['label'] = (df_copy[day] < cutoff).astype(int)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## function_feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_count(df, df_columns):\n",
    "    df_copy = df.copy()\n",
    "    df_columns_copy = df_columns.copy()\n",
    "    for idx, row in df_columns_copy.iterrows():\n",
    "        col_name = row['name']\n",
    "        df_columns_copy.loc[idx, 'availability'] = df_copy[col_name].count()/len(df_copy[col_name])\n",
    "    return df_columns_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01282024_1:\n",
    "- only delete the 0-0.01, that is nearly only one value in a column or very little value in a column\n",
    "    - threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_variance(df):\n",
    "    feature_opt = []\n",
    "    # Check whether all features have a sufficiently different meaning\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    np.shape(selector.fit_transform(df))\n",
    "    feature_opt.append(list(np.array(df.columns)[selector.get_support(indices=False)]))\n",
    "\n",
    "    return feature_opt\n",
    "\n",
    "def manual_variance(df):\n",
    "    # calculate variance\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.dropna()\n",
    "    variance = df_copy.var()\n",
    "    return variance\n",
    "\n",
    "# test a dataframe [[0, 0, 1, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 1, 1, 0], [1, 2, 3, 4]]\n",
    "X = pd.DataFrame([[0, 0, 1, 0], [0, 0, 0, 0], [0, 1, 1, 1], [0, 1, 1, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0]\n",
    "                  , [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0]\n",
    "                  , [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0], [0, 2, 4, 0]\n",
    "                  , [0, 2, 4, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_corr(df):\n",
    "    def df_corr_cal():\n",
    "        correlation = df.corr().abs().round(2)\n",
    "        return correlation\n",
    "    \n",
    "    correlation = df_corr_cal()\n",
    "    correlation_without_diagonal = correlation.mask(np.eye(len(correlation), dtype=bool))\n",
    "\n",
    "    # only keep the columns that have max correlation > 0.1\n",
    "    def df_corr_filter():\n",
    "        max_corr = correlation_without_diagonal.max()\n",
    "        features = max_corr[max_corr > 0.1].index\n",
    "        return features.tolist()\n",
    "    \n",
    "    def save_corr():\n",
    "        correlation.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_selected_correlation.tsv', sep='\\t', index=False)\n",
    "        correlation_without_diagonal.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_selected_correlation_diag.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    features = df_corr_filter()\n",
    "    save_corr()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_chi2(train, target, num_features_max=1000):\n",
    "\n",
    "    def p_value():\n",
    "        df_p_value = pd.DataFrame(columns=['variables', 'chi2', 'p-value', 'dof'])\n",
    "        for item in train.columns:\n",
    "            cross_table = pd.crosstab(train[item], target)\n",
    "            chi2, p, dof, expected = chi2_contingency(cross_table)\n",
    "            df_p_value = pd.concat([df_p_value, pd.DataFrame([[item, chi2, p, dof]], columns=['variables', 'chi2', 'p-value', 'dof'])])\n",
    "        return df_p_value\n",
    "\n",
    "    features = []\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "    fit = bestfeatures.fit(train, target)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(train.columns)\n",
    "\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Feature','Score']  #naming the dataframe columns\n",
    "    features.append(featureScores.nlargest(num_features_max,'Score')['Feature'].tolist())\n",
    "    # print(featureScores.nlargest(len(dfcolumns),'Score'))\n",
    "\n",
    "    df_p_value = p_value()\n",
    "    df_p_value = df_p_value.sort_values(by=['p-value'], ascending=True)\n",
    "\n",
    "    return features, featureScores, df_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_filter(df, p_values):\n",
    "    df_copy = df.copy()\n",
    "    # df_copy = df_copy.drop(columns=p_values_copy[p_values_copy['p-value'] > 0.05]['variables'].tolist())\n",
    "    df_copy = df_copy.drop(columns=p_values[p_values['p-value'] > 0.05]['variables'].tolist())\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_anova(group_list):\n",
    "    num_groups = len(group_list)\n",
    "    df_anova_res = pd.DataFrame(columns=['variables','F_statistic', 'p_value'])\n",
    "    group_combinations = list(itertools.combinations(range(num_groups), 2))\n",
    "    \n",
    "    for group_pair in group_combinations:\n",
    "        group1_index, group2_index = group_pair\n",
    "        group1 = group_list[group1_index]\n",
    "        group2 = group_list[group2_index]\n",
    "\n",
    "        variables = group1.columns.tolist()\n",
    "        F_statistic, p_value = f_oneway(group1, group2)\n",
    "        # save to df_anova_res\n",
    "        df_anova_res = pd.concat([df_anova_res, pd.DataFrame([[variables, F_statistic, p_value]], columns=['variables', 'F_statistic', 'p_value'])])\n",
    "     \n",
    "    return df_anova_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anova(anova_res, idx=0, p_value_cutoff=0.05):\n",
    "    # check how many values in anova_res_copy['p_value'] list that has p_value < 0.05\n",
    "    anova_res_copy = anova_res.iloc[idx].copy()\n",
    "    anova_cutoff = pd.DataFrame(columns=['variables', 'F_statistic', 'p_value'])\n",
    "    anova_cutoff['variables'] = pd.Series(anova_res_copy['variables'])\n",
    "    anova_cutoff['F_statistic'] = pd.Series(anova_res_copy['F_statistic'])\n",
    "    anova_cutoff['p_value'] = pd.Series(anova_res_copy['p_value'])\n",
    "    anova_cutoff = anova_cutoff[anova_cutoff['p_value'] < p_value_cutoff]\n",
    "    anova_cutoff = anova_cutoff.sort_values(by=['p_value'], ascending=True)\n",
    "    return anova_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_filter(df, p_values):\n",
    "    df_copy = df.copy()\n",
    "    # df_copy = df_copy.drop(columns=p_values_copy[p_values_copy['p-value'] > 0.05]['variables'].tolist())\n",
    "    # print(len(p_values['variables'].tolist()))\n",
    "    df_copy = df_copy[p_values['variables'].tolist()]\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by feature selection algorithm: mrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_mrmr(train, target):\n",
    "    features = mrmr_classif(X=train, y=target, K=10)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## function_plot drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot for continuous pandas series\n",
    "def line_plot(series):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(series)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalClassModel(model, X_test, y_test, y_pred_class, plot=False):\n",
    "\n",
    "    #Confusion matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "\n",
    "    #Confusion Matrix\n",
    "    sns.heatmap(confusion,annot=True,fmt=\"d\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    #Classification Accuracy\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    print('Classification Accuracy:', accuracy)\n",
    "\n",
    "    #Classification Error\n",
    "    print('Classification Error:', 1 - metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "    #False Positive Rate\n",
    "    false_positive_rate = FP / float(TN + FP)\n",
    "    print('False Positive Rate:', false_positive_rate)\n",
    "\n",
    "    #sensitivity(Recall)\n",
    "    sensitivity = TP/float(FN + TP)\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "\n",
    "    #specificity\n",
    "    specificity = TN/float(TN + FP)\n",
    "    print('Specificity : ', specificity)\n",
    "\n",
    "    #F1 score\n",
    "    print('F1 score:', metrics.f1_score(y_test, y_pred_class))\n",
    "\n",
    "    #Precision/ PPV\n",
    "    print('Precision(PPV):', metrics.precision_score(y_test, y_pred_class))\n",
    "\n",
    "    # AUC/ROC\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_prob = y_pred_prob.reshape(-1,1) # reshape into col vector\n",
    "    \n",
    "\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "    if plot == True:\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlim([0.0, 1.05])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.rcParams['font.size'] = 12\n",
    "        plt.title('ROC curve for treatment classifier')\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/weimin.meng/projects/AD_progression/code/0222_preliminary_results'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD_Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "showcase = pd.read_csv(SHOWCASE)\n",
    "coding = pd.read_csv(CODING)\n",
    "ad_death_70 = pd.read_csv(AD_Death_70)\n",
    "ad_death_70_clear = pd.read_csv(AD_Death_70_clear)\n",
    "ad_death_70_drop = pd.read_csv(AD_Death_70_drop)\n",
    "ad_death_80_orig = pd.read_csv(AD_Death_80_orig)\n",
    "# ad_death_80_70 = pd.read_csv(os.path.join('..', AD_Death_80_70), sep='\\t')\n",
    "ad_death_80_70_columns = pd.read_csv(AD_Death_80_70_columns, sep='\\t')\n",
    "ad_death_70_numeric = pd.read_csv(AD_Death_70_numeric, sep='\\t')\n",
    "# ad_death_70_norm = pd.read_csv(os.path.join('..', AD_Death_70_norm), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCI_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "showcase = pd.read_csv(SHOWCASE)\n",
    "coding = pd.read_csv(CODING)\n",
    "mci_ad_70 = pd.read_csv(MCI_AD_70)\n",
    "# mci_ad_70_clear = pd.read_csv(MCI_AD_70_clear)\n",
    "mci_ad_70_drop = pd.read_csv(MCI_AD_70_drop)\n",
    "mci_ad_80_orig = pd.read_csv(MCI_AD_80_orig)\n",
    "mci_ad_80_orig = pd.read_csv(MCI_AD_80_orig)\n",
    "mci_ad_80_70 = pd.read_csv(MCI_AD_80_70, sep='\\t')\n",
    "mci_ad_80_70_columns = pd.read_csv(MCI_AD_80_70_columns, sep='\\t')\n",
    "mci_ad_70_numeric = pd.read_csv(MCI_AD_70_numeric, sep='\\t')\n",
    "mci_ad_70_norm = pd.read_csv(MCI_AD_70_norm, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_list = ['held by UKB', 'assay date', 'reportability','device ID','acquisition route','acquisition time','correction reason',\n",
    "                'correction level','missing reason','Patient classification on admission (recoded)', \n",
    "                'Diagnoses - ICD10','Sources of admission to hospital (recoded)','Inpatient record format', 'authorisation',\n",
    "                'timestamp', 'CEL files', 'HLA imputation values', 'Genotype measurement batch', 'Genotype measurement plate',\n",
    "                'Present in OMOP dataset', 'Used in genetic principal components','Year of survey | Instance 0',\n",
    "                'Data points for blow | Instance 0 | Array 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name is held by UKB\n",
      "********************\n",
      "Index(['Volume of EDTA2 plasma held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 red cells held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 buffy held by UKB | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is assay date\n",
      "********************\n",
      "Index(['Vitamin D assay date | Instance 0',\n",
      "       'Total bilirubin assay date | Instance 0',\n",
      "       'Triglycerides assay date | Instance 0',\n",
      "       'Rheumatoid factor assay date | Instance 0',\n",
      "       'Total protein assay date | Instance 0',\n",
      "       'LDL direct assay date | Instance 0',\n",
      "       'Phosphate assay date | Instance 0', 'IGF-1 assay date | Instance 0',\n",
      "       'Urate assay date | Instance 0', 'Testosterone assay date | Instance 0',\n",
      "       'SHBG assay date | Instance 0', 'Lipoprotein A assay date | Instance 0',\n",
      "       'Oestradiol assay date | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is reportability\n",
      "********************\n",
      "Index(['Triglycerides reportability | Instance 0',\n",
      "       'Rheumatoid factor reportability | Instance 0',\n",
      "       'LDL direct reportability | Instance 0',\n",
      "       'Total bilirubin reportability | Instance 0',\n",
      "       'Urate reportability | Instance 0', 'IGF-1 reportability | Instance 0',\n",
      "       'Testosterone reportability | Instance 0',\n",
      "       'Lipoprotein A reportability | Instance 0',\n",
      "       'Vitamin D reportability | Instance 0',\n",
      "       'Oestradiol reportability | Instance 0',\n",
      "       'Total protein reportability | Instance 0',\n",
      "       'Phosphate reportability | Instance 0',\n",
      "       'HDL cholesterol reportability | Instance 0',\n",
      "       'SHBG reportability | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is device ID\n",
      "********************\n",
      "Index(['Blood pressure device ID | Instance 0',\n",
      "       'Height measure device ID | Instance 0',\n",
      "       'Seating box device ID | Instance 0',\n",
      "       'Tape measure device ID | Instance 0',\n",
      "       'Impedance device ID | Instance 0',\n",
      "       'Hand grip dynamometer device ID | Instance 0',\n",
      "       'Spirometer device ID | Instance 0',\n",
      "       'Manual scales device ID | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is acquisition route\n",
      "********************\n",
      "Index(['Haematocrit percentage acquisition route | Instance 0', 'Mean sphered cell volume acquisition route | Instance 0'], dtype='object')\n",
      "column name is acquisition time\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is correction reason\n",
      "********************\n",
      "Index(['Alkaline phosphatase correction reason | Instance 0',\n",
      "       'Albumin correction reason | Instance 0',\n",
      "       'Gamma glutamyltransferase correction reason | Instance 0',\n",
      "       'Cholesterol correction reason | Instance 0',\n",
      "       'Alanine aminotransferase correction reason | Instance 0',\n",
      "       'Urea correction reason | Instance 0',\n",
      "       'Cystatin C correction reason | Instance 0',\n",
      "       'Creatinine correction reason | Instance 0',\n",
      "       'C-reactive protein correction reason | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) correction reason | Instance 0',\n",
      "       'Aspartate aminotransferase correction reason | Instance 0',\n",
      "       'Apolipoprotein B correction reason | Instance 0',\n",
      "       'Calcium correction reason | Instance 0',\n",
      "       'HDL cholesterol correction reason | Instance 0',\n",
      "       'Glucose correction reason | Instance 0',\n",
      "       'Apolipoprotein A correction reason | Instance 0',\n",
      "       'Direct bilirubin correction reason | Instance 0',\n",
      "       'Triglycerides correction reason | Instance 0',\n",
      "       'LDL direct correction reason | Instance 0',\n",
      "       'Total bilirubin correction reason | Instance 0',\n",
      "       'IGF-1 correction reason | Instance 0',\n",
      "       'Urate correction reason | Instance 0',\n",
      "       'Vitamin D correction reason | Instance 0',\n",
      "       'Total protein correction reason | Instance 0',\n",
      "       'Phosphate correction reason | Instance 0',\n",
      "       'SHBG correction reason | Instance 0',\n",
      "       'Testosterone correction reason | Instance 0',\n",
      "       'Lipoprotein A correction reason | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is correction level\n",
      "********************\n",
      "Index(['Alkaline phosphatase correction level | Instance 0',\n",
      "       'Albumin correction level | Instance 0',\n",
      "       'Cholesterol correction level | Instance 0',\n",
      "       'Alanine aminotransferase correction level | Instance 0',\n",
      "       'Urea correction level | Instance 0',\n",
      "       'Creatinine correction level | Instance 0',\n",
      "       'Cystatin C correction level | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) correction level | Instance 0',\n",
      "       'Aspartate aminotransferase correction level | Instance 0',\n",
      "       'Apolipoprotein B correction level | Instance 0',\n",
      "       'Calcium correction level | Instance 0',\n",
      "       'HDL cholesterol correction level | Instance 0',\n",
      "       'Glucose correction level | Instance 0',\n",
      "       'Apolipoprotein A correction level | Instance 0',\n",
      "       'Direct bilirubin correction level | Instance 0',\n",
      "       'Triglycerides correction level | Instance 0',\n",
      "       'LDL direct correction level | Instance 0',\n",
      "       'Total bilirubin correction level | Instance 0',\n",
      "       'IGF-1 correction level | Instance 0',\n",
      "       'Urate correction level | Instance 0',\n",
      "       'Total protein correction level | Instance 0',\n",
      "       'Phosphate correction level | Instance 0',\n",
      "       'SHBG correction level | Instance 0',\n",
      "       'Testosterone correction level | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is missing reason\n",
      "********************\n",
      "Index(['Oestradiol missing reason | Instance 0', 'Rheumatoid factor missing reason | Instance 0'], dtype='object')\n",
      "column name is Patient classification on admission (recoded)\n",
      "********************\n",
      "Index(['Patient classification on admission (recoded)'], dtype='object')\n",
      "column name is Diagnoses - ICD10\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Sources of admission to hospital (recoded)\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Inpatient record format\n",
      "********************\n",
      "Index(['Inpatient record format'], dtype='object')\n",
      "column name is authorisation\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is timestamp\n",
      "********************\n",
      "Index(['Program (tactus) version ID (compiler timestamp)'], dtype='object')\n",
      "column name is CEL files\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is HLA imputation values\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Genotype measurement batch\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Genotype measurement plate\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Present in OMOP dataset\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Used in genetic principal components\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Year of survey | Instance 0\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Data points for blow | Instance 0 | Array 0\n",
      "********************\n",
      "Index(['Data points for blow | Instance 0 | Array 0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ad_death_70_clear_drop = column_delete(ad_death_70_clear, delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name is held by UKB\n",
      "********************\n",
      "Index(['Volume of EDTA1 red cells held by UKB | Instance 0',\n",
      "       'Volume of EDTA1 plasma held by UKB | Instance 0',\n",
      "       'Volume of EDTA1 buffy held by UKB | Instance 0',\n",
      "       'Volume of Li-Hep plasma held by UKB | Instance 0',\n",
      "       'Volume of ACD held by UKB | Instance 0',\n",
      "       'Volume of serum held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 plasma held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 red cells held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 buffy held by UKB | Instance 0',\n",
      "       'Total volume of urine samples held by UKB | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is assay date\n",
      "********************\n",
      "Index(['Alkaline phosphatase assay date | Instance 0',\n",
      "       'Albumin assay date | Instance 0',\n",
      "       'Alanine aminotransferase assay date | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) assay date | Instance 0',\n",
      "       'Urea assay date | Instance 0', 'Cholesterol assay date | Instance 0',\n",
      "       'C-reactive protein assay date | Instance 0',\n",
      "       'Cystatin C assay date | Instance 0',\n",
      "       'HDL cholesterol assay date | Instance 0',\n",
      "       'Creatinine assay date | Instance 0',\n",
      "       'Apolipoprotein B assay date | Instance 0',\n",
      "       'Calcium assay date | Instance 0',\n",
      "       'Gamma glutamyltransferase assay date | Instance 0',\n",
      "       'Aspartate aminotransferase assay date | Instance 0',\n",
      "       'Glucose assay date | Instance 0',\n",
      "       'Apolipoprotein A assay date | Instance 0',\n",
      "       'Direct bilirubin assay date | Instance 0',\n",
      "       'Triglycerides assay date | Instance 0',\n",
      "       'Total bilirubin assay date | Instance 0',\n",
      "       'Vitamin D assay date | Instance 0', 'Urate assay date | Instance 0',\n",
      "       'LDL direct assay date | Instance 0', 'SHBG assay date | Instance 0',\n",
      "       'Phosphate assay date | Instance 0',\n",
      "       'Total protein assay date | Instance 0',\n",
      "       'Rheumatoid factor assay date | Instance 0',\n",
      "       'Testosterone assay date | Instance 0', 'IGF-1 assay date | Instance 0',\n",
      "       'Lipoprotein A assay date | Instance 0',\n",
      "       'Oestradiol assay date | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is reportability\n",
      "********************\n",
      "Index(['Alkaline phosphatase reportability | Instance 0',\n",
      "       'Albumin reportability | Instance 0',\n",
      "       'Creatinine reportability | Instance 0',\n",
      "       'Cholesterol reportability | Instance 0',\n",
      "       'Cystatin C reportability | Instance 0',\n",
      "       'Apolipoprotein B reportability | Instance 0',\n",
      "       'C-reactive protein reportability | Instance 0',\n",
      "       'Alanine aminotransferase reportability | Instance 0',\n",
      "       'Urea reportability | Instance 0',\n",
      "       'Gamma glutamyltransferase reportability | Instance 0',\n",
      "       'Aspartate aminotransferase reportability | Instance 0',\n",
      "       'Direct bilirubin reportability | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) reportability | Instance 0',\n",
      "       'Calcium reportability | Instance 0',\n",
      "       'Glucose reportability | Instance 0',\n",
      "       'Apolipoprotein A reportability | Instance 0',\n",
      "       'Rheumatoid factor reportability | Instance 0',\n",
      "       'Urate reportability | Instance 0',\n",
      "       'Triglycerides reportability | Instance 0',\n",
      "       'LDL direct reportability | Instance 0',\n",
      "       'Total bilirubin reportability | Instance 0',\n",
      "       'Testosterone reportability | Instance 0',\n",
      "       'IGF-1 reportability | Instance 0',\n",
      "       'Lipoprotein A reportability | Instance 0',\n",
      "       'Vitamin D reportability | Instance 0',\n",
      "       'HDL cholesterol reportability | Instance 0',\n",
      "       'Total protein reportability | Instance 0',\n",
      "       'SHBG reportability | Instance 0',\n",
      "       'Phosphate reportability | Instance 0',\n",
      "       'Oestradiol reportability | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is device ID\n",
      "********************\n",
      "Index(['Seating box device ID | Instance 0',\n",
      "       'Tape measure device ID | Instance 0',\n",
      "       'Blood pressure device ID | Instance 0',\n",
      "       'Impedance device ID | Instance 0',\n",
      "       'Height measure device ID | Instance 0',\n",
      "       'Hand grip dynamometer device ID | Instance 0',\n",
      "       'Manual scales device ID | Instance 0',\n",
      "       'Spirometer device ID | Instance 0',\n",
      "       'Neutrophill count device ID | Instance 0',\n",
      "       'Nucleated red blood cell count device ID | Instance 0',\n",
      "       'Red blood cell (erythrocyte) distribution width device ID | Instance 0',\n",
      "       'Mean corpuscular volume device ID | Instance 0',\n",
      "       'Eosinophill count device ID | Instance 0',\n",
      "       'Haematocrit percentage device ID | Instance 0',\n",
      "       'Platelet crit device ID | Instance 0',\n",
      "       'Basophill count device ID | Instance 0',\n",
      "       'Mean corpuscular haemoglobin concentration device ID | Instance 0',\n",
      "       'Monocyte count device ID | Instance 0',\n",
      "       'Mean corpuscular haemoglobin device ID | Instance 0',\n",
      "       'Platelet distribution width device ID | Instance 0',\n",
      "       'Mean platelet (thrombocyte) volume device ID | Instance 0',\n",
      "       'Red blood cell (erythrocyte) count device ID | Instance 0',\n",
      "       'Lymphocyte percentage device ID | Instance 0',\n",
      "       'White blood cell (leukocyte) count device ID | Instance 0',\n",
      "       'Lymphocyte count device ID | Instance 0',\n",
      "       'Platelet count device ID | Instance 0',\n",
      "       'Haemoglobin concentration device ID | Instance 0',\n",
      "       'Monocyte percentage device ID | Instance 0',\n",
      "       'Eosinophill percentage device ID | Instance 0',\n",
      "       'Nucleated red blood cell percentage device ID | Instance 0',\n",
      "       'Neutrophill percentage device ID | Instance 0',\n",
      "       'Basophill percentage device ID | Instance 0',\n",
      "       'High light scatter reticulocyte count device ID | Instance 0',\n",
      "       'Immature reticulocyte fraction device ID | Instance 0',\n",
      "       'Reticulocyte percentage device ID | Instance 0',\n",
      "       'High light scatter reticulocyte percentage device ID | Instance 0',\n",
      "       'Mean reticulocyte volume device ID | Instance 0',\n",
      "       'Reticulocyte count device ID | Instance 0',\n",
      "       'Mean sphered cell volume device ID | Instance 0',\n",
      "       'Sodium in urine device ID | Instance 0',\n",
      "       'Creatinine (enzymatic) in urine device ID | Instance 0',\n",
      "       'Microalbumin in urine device ID | Instance 0',\n",
      "       'Potassium in urine device ID | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is acquisition route\n",
      "********************\n",
      "Index(['Red blood cell (erythrocyte) count acquisition route | Instance 0',\n",
      "       'Red blood cell (erythrocyte) distribution width acquisition route | Instance 0',\n",
      "       'Platelet crit acquisition route | Instance 0',\n",
      "       'Monocyte percentage acquisition route | Instance 0',\n",
      "       'Nucleated red blood cell count acquisition route | Instance 0',\n",
      "       'Mean platelet (thrombocyte) volume acquisition route | Instance 0',\n",
      "       'Haematocrit percentage acquisition route | Instance 0',\n",
      "       'Basophill count acquisition route | Instance 0',\n",
      "       'Mean corpuscular haemoglobin acquisition route | Instance 0',\n",
      "       'Mean corpuscular haemoglobin concentration acquisition route | Instance 0',\n",
      "       'Platelet count acquisition route | Instance 0',\n",
      "       'Neutrophill count acquisition route | Instance 0',\n",
      "       'Mean corpuscular volume acquisition route | Instance 0',\n",
      "       'Eosinophill count acquisition route | Instance 0',\n",
      "       'Platelet distribution width acquisition route | Instance 0',\n",
      "       'White blood cell (leukocyte) count acquisition route | Instance 0',\n",
      "       'Lymphocyte count acquisition route | Instance 0',\n",
      "       'Monocyte count acquisition route | Instance 0',\n",
      "       'Haemoglobin concentration acquisition route | Instance 0',\n",
      "       'Lymphocyte percentage acquisition route | Instance 0',\n",
      "       'Basophill percentage acquisition route | Instance 0',\n",
      "       'Eosinophill percentage acquisition route | Instance 0',\n",
      "       'Neutrophill percentage acquisition route | Instance 0',\n",
      "       'Nucleated red blood cell percentage acquisition route | Instance 0',\n",
      "       'Mean sphered cell volume acquisition route | Instance 0',\n",
      "       'Immature reticulocyte fraction acquisition route | Instance 0',\n",
      "       'High light scatter reticulocyte count acquisition route | Instance 0',\n",
      "       'Mean reticulocyte volume acquisition route | Instance 0',\n",
      "       'Reticulocyte count acquisition route | Instance 0',\n",
      "       'High light scatter reticulocyte percentage acquisition route | Instance 0',\n",
      "       'Reticulocyte percentage acquisition route | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is acquisition time\n",
      "********************\n",
      "Index(['Red blood cell (erythrocyte) count acquisition time | Instance 0',\n",
      "       'Basophill count acquisition time | Instance 0',\n",
      "       'Red blood cell (erythrocyte) distribution width acquisition time | Instance 0',\n",
      "       'Mean corpuscular haemoglobin acquisition time | Instance 0',\n",
      "       'Platelet count acquisition time | Instance 0',\n",
      "       'Eosinophill count acquisition time | Instance 0',\n",
      "       'Lymphocyte percentage acquisition time | Instance 0',\n",
      "       'Nucleated red blood cell count acquisition time | Instance 0',\n",
      "       'Lymphocyte count acquisition time | Instance 0',\n",
      "       'Neutrophill count acquisition time | Instance 0',\n",
      "       'Mean corpuscular volume acquisition time | Instance 0',\n",
      "       'Platelet crit acquisition time | Instance 0',\n",
      "       'Monocyte percentage acquisition time | Instance 0',\n",
      "       'Mean platelet (thrombocyte) volume acquisition time | Instance 0',\n",
      "       'Platelet distribution width acquisition time | Instance 0',\n",
      "       'White blood cell (leukocyte) count acquisition time | Instance 0',\n",
      "       'Haematocrit acquisition time | Instance 0',\n",
      "       'Mean corpuscular haemoglobin concentration acquisition time | Instance 0',\n",
      "       'Haemoglobin concentration acquisition time | Instance 0',\n",
      "       'Monocyte count acquisition time | Instance 0',\n",
      "       'Nucleated red blood cell percentage acquisition time | Instance 0',\n",
      "       'Eosinophill percentage acquisition time | Instance 0',\n",
      "       'Neutrophill percentage acquisition time | Instance 0',\n",
      "       'Basophill percentage acquisition time | Instance 0',\n",
      "       'Immature reticulocyte fraction acquisition time | Instance 0',\n",
      "       'Reticulocyte percentage acquisition time | Instance 0',\n",
      "       'Mean reticulocyte volume acquisition time | Instance 0',\n",
      "       'High light scatter reticulocyte count acquisition time | Instance 0',\n",
      "       'Reticulocyte count acquisition time | Instance 0',\n",
      "       'High light scatter reticulocyte percentage acquisition time | Instance 0',\n",
      "       'Mean sphered cell volume acquisition time | Instance 0',\n",
      "       'Creatinine (enzymatic) in urine acquisition time | Instance 0',\n",
      "       'Microalbumin in urine acquisition time | Instance 0',\n",
      "       'Potassium in urine acquisition time | Instance 0',\n",
      "       'Sodium in urine acquisition time | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is correction reason\n",
      "********************\n",
      "Index(['Alkaline phosphatase correction reason | Instance 0',\n",
      "       'Albumin correction reason | Instance 0',\n",
      "       'Creatinine correction reason | Instance 0',\n",
      "       'Cystatin C correction reason | Instance 0',\n",
      "       'C-reactive protein correction reason | Instance 0',\n",
      "       'Apolipoprotein B correction reason | Instance 0',\n",
      "       'Alanine aminotransferase correction reason | Instance 0',\n",
      "       'Urea correction reason | Instance 0',\n",
      "       'Gamma glutamyltransferase correction reason | Instance 0',\n",
      "       'Cholesterol correction reason | Instance 0',\n",
      "       'Aspartate aminotransferase correction reason | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) correction reason | Instance 0',\n",
      "       'Calcium correction reason | Instance 0',\n",
      "       'Glucose correction reason | Instance 0',\n",
      "       'HDL cholesterol correction reason | Instance 0',\n",
      "       'Apolipoprotein A correction reason | Instance 0',\n",
      "       'Direct bilirubin correction reason | Instance 0',\n",
      "       'Triglycerides correction reason | Instance 0',\n",
      "       'Urate correction reason | Instance 0',\n",
      "       'LDL direct correction reason | Instance 0',\n",
      "       'Total bilirubin correction reason | Instance 0',\n",
      "       'IGF-1 correction reason | Instance 0',\n",
      "       'Vitamin D correction reason | Instance 0',\n",
      "       'Phosphate correction reason | Instance 0',\n",
      "       'Testosterone correction reason | Instance 0',\n",
      "       'SHBG correction reason | Instance 0',\n",
      "       'Total protein correction reason | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is correction level\n",
      "********************\n",
      "Index(['Alkaline phosphatase correction level | Instance 0',\n",
      "       'Albumin correction level | Instance 0',\n",
      "       'Urea correction level | Instance 0',\n",
      "       'Cholesterol correction level | Instance 0',\n",
      "       'Alanine aminotransferase correction level | Instance 0',\n",
      "       'Cystatin C correction level | Instance 0',\n",
      "       'Apolipoprotein B correction level | Instance 0',\n",
      "       'Creatinine correction level | Instance 0',\n",
      "       'Aspartate aminotransferase correction level | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) correction level | Instance 0',\n",
      "       'Calcium correction level | Instance 0',\n",
      "       'Glucose correction level | Instance 0',\n",
      "       'HDL cholesterol correction level | Instance 0',\n",
      "       'Apolipoprotein A correction level | Instance 0',\n",
      "       'Direct bilirubin correction level | Instance 0',\n",
      "       'LDL direct correction level | Instance 0',\n",
      "       'Urate correction level | Instance 0',\n",
      "       'Triglycerides correction level | Instance 0',\n",
      "       'Total bilirubin correction level | Instance 0',\n",
      "       'IGF-1 correction level | Instance 0',\n",
      "       'Total protein correction level | Instance 0',\n",
      "       'Phosphate correction level | Instance 0',\n",
      "       'SHBG correction level | Instance 0',\n",
      "       'Testosterone correction level | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is missing reason\n",
      "********************\n",
      "Index(['Oestradiol missing reason | Instance 0', 'Rheumatoid factor missing reason | Instance 0'], dtype='object')\n",
      "column name is Patient classification on admission (recoded)\n",
      "********************\n",
      "Index(['Patient classification on admission (recoded)'], dtype='object')\n",
      "column name is Diagnoses - ICD10\n",
      "********************\n",
      "Index(['Diagnoses - ICD10'], dtype='object')\n",
      "column name is Sources of admission to hospital (recoded)\n",
      "********************\n",
      "Index(['Sources of admission to hospital (recoded)'], dtype='object')\n",
      "column name is Inpatient record format\n",
      "********************\n",
      "Index(['Inpatient record format'], dtype='object')\n",
      "column name is authorisation\n",
      "********************\n",
      "Index(['Touchscreen authorisation | Instance 0',\n",
      "       'Conclusion authorisation | Instance 0',\n",
      "       'Reception authorisation | Instance 0',\n",
      "       'Consent authorisation | Instance 0',\n",
      "       'Sample collection authorisation | Instance 0',\n",
      "       'Verbal interview authorisation | Instance 0',\n",
      "       'Biometrics authorisation | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is timestamp\n",
      "********************\n",
      "Index(['Program (tactus) version ID (compiler timestamp)',\n",
      "       'Reception sign-off timestamp | Instance 0',\n",
      "       'Touchscreen sign-off timestamp | Instance 0',\n",
      "       'Consent sign-off timestamp | Instance 0',\n",
      "       'Conclusion sign-off timestamp | Instance 0',\n",
      "       'Sample collection sign-off timestamp | Instance 0',\n",
      "       'Verbal nterview sign-off timestamp | Instance 0',\n",
      "       'Biometrics sign-off timestamp | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is CEL files\n",
      "********************\n",
      "Index(['CEL files'], dtype='object')\n",
      "column name is HLA imputation values\n",
      "********************\n",
      "Index(['HLA imputation values'], dtype='object')\n",
      "column name is Genotype measurement batch\n",
      "********************\n",
      "Index(['Genotype measurement batch'], dtype='object')\n",
      "column name is Genotype measurement plate\n",
      "********************\n",
      "Index(['Genotype measurement plate'], dtype='object')\n",
      "column name is Present in OMOP dataset\n",
      "********************\n",
      "Index(['Present in OMOP dataset'], dtype='object')\n",
      "column name is Used in genetic principal components\n",
      "********************\n",
      "Index(['Used in genetic principal components'], dtype='object')\n",
      "column name is Year of survey | Instance 0\n",
      "********************\n",
      "Index(['Year of survey | Instance 0'], dtype='object')\n",
      "column name is Data points for blow | Instance 0 | Array 0\n",
      "********************\n",
      "Index(['Data points for blow | Instance 0 | Array 0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "mci_ad_70_clear_drop = column_delete(mci_ad_70, delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_difference(ad_death_70, ad_death_70_clear_drop, 'data/weimin/01282024/delete_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cholesterol correction reason | Instance 0',\n",
       " 'Volume of EDTA1 red cells held by UKB | Instance 0',\n",
       " 'Testosterone reportability | Instance 0',\n",
       " 'Nucleated red blood cell percentage device ID | Instance 0',\n",
       " 'Lymphocyte percentage acquisition time | Instance 0',\n",
       " 'Glycated haemoglobin (HbA1c) correction level | Instance 0',\n",
       " 'HDL cholesterol correction level | Instance 0',\n",
       " 'Red blood cell (erythrocyte) distribution width acquisition route | Instance 0',\n",
       " 'Platelet crit device ID | Instance 0',\n",
       " 'SHBG correction reason | Instance 0',\n",
       " 'Touchscreen sign-off timestamp | Instance 0',\n",
       " 'Volume of serum held by UKB | Instance 0',\n",
       " 'Eosinophill percentage acquisition route | Instance 0',\n",
       " 'Monocyte percentage device ID | Instance 0',\n",
       " 'High light scatter reticulocyte count acquisition time | Instance 0',\n",
       " 'HDL cholesterol reportability | Instance 0',\n",
       " 'Reticulocyte percentage device ID | Instance 0',\n",
       " 'Neutrophill count acquisition time | Instance 0',\n",
       " 'Rheumatoid factor missing reason | Instance 0',\n",
       " 'High light scatter reticulocyte percentage acquisition time | Instance 0',\n",
       " 'Creatinine assay date | Instance 0',\n",
       " 'Patient classification on admission (recoded)',\n",
       " 'Urea assay date | Instance 0',\n",
       " 'Gamma glutamyltransferase assay date | Instance 0',\n",
       " 'Eosinophill percentage acquisition time | Instance 0',\n",
       " 'Haemoglobin concentration acquisition route | Instance 0',\n",
       " 'Reception sign-off timestamp | Instance 0',\n",
       " 'Microalbumin in urine acquisition time | Instance 0',\n",
       " 'Oestradiol reportability | Instance 0',\n",
       " 'Albumin correction reason | Instance 0',\n",
       " 'Mean corpuscular haemoglobin acquisition route | Instance 0',\n",
       " 'Glycated haemoglobin (HbA1c) assay date | Instance 0',\n",
       " 'White blood cell (leukocyte) count acquisition route | Instance 0',\n",
       " 'Eosinophill count acquisition time | Instance 0',\n",
       " 'Oestradiol assay date | Instance 0',\n",
       " 'HDL cholesterol correction reason | Instance 0',\n",
       " 'Mean corpuscular haemoglobin concentration device ID | Instance 0',\n",
       " 'Neutrophill percentage acquisition route | Instance 0',\n",
       " 'Vitamin D assay date | Instance 0',\n",
       " 'Sodium in urine device ID | Instance 0',\n",
       " 'Platelet distribution width device ID | Instance 0',\n",
       " 'Mean reticulocyte volume acquisition time | Instance 0',\n",
       " 'Reticulocyte count device ID | Instance 0',\n",
       " 'Haematocrit percentage device ID | Instance 0',\n",
       " 'Glucose reportability | Instance 0',\n",
       " 'Glycated haemoglobin (HbA1c) correction reason | Instance 0',\n",
       " 'Total volume of urine samples held by UKB | Instance 0',\n",
       " 'Creatinine reportability | Instance 0',\n",
       " 'Total protein reportability | Instance 0',\n",
       " 'Creatinine correction level | Instance 0',\n",
       " 'LDL direct correction level | Instance 0',\n",
       " 'Immature reticulocyte fraction device ID | Instance 0',\n",
       " 'Apolipoprotein B assay date | Instance 0',\n",
       " 'White blood cell (leukocyte) count device ID | Instance 0',\n",
       " 'Haematocrit acquisition time | Instance 0',\n",
       " 'Apolipoprotein B reportability | Instance 0',\n",
       " 'Alanine aminotransferase correction reason | Instance 0',\n",
       " 'Immature reticulocyte fraction acquisition time | Instance 0',\n",
       " 'Platelet count acquisition route | Instance 0',\n",
       " 'Basophill percentage device ID | Instance 0',\n",
       " 'Impedance device ID | Instance 0',\n",
       " 'Nucleated red blood cell count device ID | Instance 0',\n",
       " 'IGF-1 correction level | Instance 0',\n",
       " 'Genotype measurement plate',\n",
       " 'Present in OMOP dataset',\n",
       " 'High light scatter reticulocyte percentage acquisition route | Instance 0',\n",
       " 'Triglycerides reportability | Instance 0',\n",
       " 'C-reactive protein correction reason | Instance 0',\n",
       " 'Mean platelet (thrombocyte) volume acquisition time | Instance 0',\n",
       " 'Urate correction level | Instance 0',\n",
       " 'Aspartate aminotransferase reportability | Instance 0',\n",
       " 'SHBG assay date | Instance 0',\n",
       " 'Albumin reportability | Instance 0',\n",
       " 'Red blood cell (erythrocyte) distribution width acquisition time | Instance 0',\n",
       " 'Apolipoprotein B correction reason | Instance 0',\n",
       " 'Albumin correction level | Instance 0',\n",
       " 'Aspartate aminotransferase assay date | Instance 0',\n",
       " 'Volume of EDTA2 buffy held by UKB | Instance 0',\n",
       " 'Basophill percentage acquisition route | Instance 0',\n",
       " 'Lymphocyte count device ID | Instance 0',\n",
       " 'Apolipoprotein B correction level | Instance 0',\n",
       " 'C-reactive protein assay date | Instance 0',\n",
       " 'Genotype measurement batch',\n",
       " 'Platelet count device ID | Instance 0',\n",
       " 'Mean corpuscular haemoglobin acquisition time | Instance 0',\n",
       " 'Neutrophill percentage device ID | Instance 0',\n",
       " 'Direct bilirubin correction reason | Instance 0',\n",
       " 'Basophill count acquisition time | Instance 0',\n",
       " 'Sources of admission to hospital (recoded)',\n",
       " 'LDL direct correction reason | Instance 0',\n",
       " 'Nucleated red blood cell count acquisition route | Instance 0',\n",
       " 'Monocyte percentage acquisition time | Instance 0',\n",
       " 'Aspartate aminotransferase correction level | Instance 0',\n",
       " 'Biometrics sign-off timestamp | Instance 0',\n",
       " 'Sodium in urine acquisition time | Instance 0',\n",
       " 'HDL cholesterol assay date | Instance 0',\n",
       " 'Volume of ACD held by UKB | Instance 0',\n",
       " 'Urea reportability | Instance 0',\n",
       " 'White blood cell (leukocyte) count acquisition time | Instance 0',\n",
       " 'Triglycerides assay date | Instance 0',\n",
       " 'Platelet count acquisition time | Instance 0',\n",
       " 'Immature reticulocyte fraction acquisition route | Instance 0',\n",
       " 'Volume of EDTA1 plasma held by UKB | Instance 0',\n",
       " 'Apolipoprotein A assay date | Instance 0',\n",
       " 'Height measure device ID | Instance 0',\n",
       " 'Urea correction reason | Instance 0',\n",
       " 'Tape measure device ID | Instance 0',\n",
       " 'Lymphocyte count acquisition time | Instance 0',\n",
       " 'Basophill count device ID | Instance 0',\n",
       " 'Urate correction reason | Instance 0',\n",
       " 'LDL direct reportability | Instance 0',\n",
       " 'Total bilirubin correction reason | Instance 0',\n",
       " 'Monocyte percentage acquisition route | Instance 0',\n",
       " 'Monocyte count device ID | Instance 0',\n",
       " 'Cholesterol assay date | Instance 0',\n",
       " 'Total bilirubin reportability | Instance 0',\n",
       " 'Microalbumin in urine device ID | Instance 0',\n",
       " 'Glucose correction reason | Instance 0',\n",
       " 'Red blood cell (erythrocyte) count acquisition time | Instance 0',\n",
       " 'Gamma glutamyltransferase correction reason | Instance 0',\n",
       " 'IGF-1 assay date | Instance 0',\n",
       " 'Mean corpuscular haemoglobin concentration acquisition time | Instance 0',\n",
       " 'Platelet crit acquisition time | Instance 0',\n",
       " 'Oestradiol missing reason | Instance 0',\n",
       " 'Alkaline phosphatase correction reason | Instance 0',\n",
       " 'Red blood cell (erythrocyte) count device ID | Instance 0',\n",
       " 'Creatinine correction reason | Instance 0',\n",
       " 'Total protein correction reason | Instance 0',\n",
       " 'Nucleated red blood cell percentage acquisition time | Instance 0',\n",
       " 'Cholesterol correction level | Instance 0',\n",
       " 'Albumin assay date | Instance 0',\n",
       " 'Volume of Li-Hep plasma held by UKB | Instance 0',\n",
       " 'Phosphate correction reason | Instance 0',\n",
       " 'Lipoprotein A assay date | Instance 0',\n",
       " 'Total protein assay date | Instance 0',\n",
       " 'Verbal interview authorisation | Instance 0',\n",
       " 'Direct bilirubin assay date | Instance 0',\n",
       " 'Phosphate correction level | Instance 0',\n",
       " 'Creatinine (enzymatic) in urine device ID | Instance 0',\n",
       " 'Glucose assay date | Instance 0',\n",
       " 'Biometrics authorisation | Instance 0',\n",
       " 'Direct bilirubin reportability | Instance 0',\n",
       " 'Mean sphered cell volume acquisition route | Instance 0',\n",
       " 'High light scatter reticulocyte percentage device ID | Instance 0',\n",
       " 'Total bilirubin assay date | Instance 0',\n",
       " 'Conclusion sign-off timestamp | Instance 0',\n",
       " 'Phosphate assay date | Instance 0',\n",
       " 'Hand grip dynamometer device ID | Instance 0',\n",
       " 'Mean platelet (thrombocyte) volume acquisition route | Instance 0',\n",
       " 'Triglycerides correction level | Instance 0',\n",
       " 'Consent authorisation | Instance 0',\n",
       " 'SHBG reportability | Instance 0',\n",
       " 'High light scatter reticulocyte count device ID | Instance 0',\n",
       " 'Alanine aminotransferase assay date | Instance 0',\n",
       " 'Mean sphered cell volume acquisition time | Instance 0',\n",
       " 'Red blood cell (erythrocyte) distribution width device ID | Instance 0',\n",
       " 'Eosinophill count acquisition route | Instance 0',\n",
       " 'Calcium assay date | Instance 0',\n",
       " 'Neutrophill count acquisition route | Instance 0',\n",
       " 'Alkaline phosphatase correction level | Instance 0',\n",
       " 'Sample collection authorisation | Instance 0',\n",
       " 'Nucleated red blood cell percentage acquisition route | Instance 0',\n",
       " 'Neutrophill percentage acquisition time | Instance 0',\n",
       " 'Calcium correction level | Instance 0',\n",
       " 'Cholesterol reportability | Instance 0',\n",
       " 'Manual scales device ID | Instance 0',\n",
       " 'Mean corpuscular volume acquisition time | Instance 0',\n",
       " 'Testosterone correction reason | Instance 0',\n",
       " 'Total protein correction level | Instance 0',\n",
       " 'Neutrophill count device ID | Instance 0',\n",
       " 'Seating box device ID | Instance 0',\n",
       " 'Touchscreen authorisation | Instance 0',\n",
       " 'Alkaline phosphatase reportability | Instance 0',\n",
       " 'Basophill percentage acquisition time | Instance 0',\n",
       " 'Lymphocyte percentage device ID | Instance 0',\n",
       " 'Phosphate reportability | Instance 0',\n",
       " 'Triglycerides correction reason | Instance 0',\n",
       " 'Reticulocyte count acquisition route | Instance 0',\n",
       " 'Alkaline phosphatase assay date | Instance 0',\n",
       " 'Program (tactus) version ID (compiler timestamp)',\n",
       " 'Mean sphered cell volume device ID | Instance 0',\n",
       " 'Direct bilirubin correction level | Instance 0',\n",
       " 'Urate assay date | Instance 0',\n",
       " 'IGF-1 correction reason | Instance 0',\n",
       " 'Cystatin C reportability | Instance 0',\n",
       " 'HLA imputation values',\n",
       " 'Apolipoprotein A correction reason | Instance 0',\n",
       " 'Volume of EDTA2 plasma held by UKB | Instance 0',\n",
       " 'Lymphocyte count acquisition route | Instance 0',\n",
       " 'Basophill count acquisition route | Instance 0',\n",
       " 'Mean corpuscular haemoglobin concentration acquisition route | Instance 0',\n",
       " 'Used in genetic principal components',\n",
       " 'IGF-1 reportability | Instance 0',\n",
       " 'Year of survey | Instance 0',\n",
       " 'Mean corpuscular volume device ID | Instance 0',\n",
       " 'Platelet distribution width acquisition time | Instance 0',\n",
       " 'Eosinophill percentage device ID | Instance 0',\n",
       " 'Apolipoprotein A correction level | Instance 0',\n",
       " 'Creatinine (enzymatic) in urine acquisition time | Instance 0',\n",
       " 'Haemoglobin concentration device ID | Instance 0',\n",
       " 'Alanine aminotransferase correction level | Instance 0',\n",
       " 'Monocyte count acquisition time | Instance 0',\n",
       " 'Mean corpuscular haemoglobin device ID | Instance 0',\n",
       " 'Rheumatoid factor reportability | Instance 0',\n",
       " 'Volume of EDTA1 buffy held by UKB | Instance 0',\n",
       " 'Testosterone assay date | Instance 0',\n",
       " 'Diagnoses - ICD10',\n",
       " 'Volume of EDTA2 red cells held by UKB | Instance 0',\n",
       " 'SHBG correction level | Instance 0',\n",
       " 'Monocyte count acquisition route | Instance 0',\n",
       " 'Vitamin D reportability | Instance 0',\n",
       " 'Glycated haemoglobin (HbA1c) reportability | Instance 0',\n",
       " 'Sample collection sign-off timestamp | Instance 0',\n",
       " 'Inpatient record format',\n",
       " 'Conclusion authorisation | Instance 0',\n",
       " 'Reception authorisation | Instance 0',\n",
       " 'Consent sign-off timestamp | Instance 0',\n",
       " 'Aspartate aminotransferase correction reason | Instance 0',\n",
       " 'Total bilirubin correction level | Instance 0',\n",
       " 'Vitamin D correction reason | Instance 0',\n",
       " 'Eosinophill count device ID | Instance 0',\n",
       " 'Urea correction level | Instance 0',\n",
       " 'High light scatter reticulocyte count acquisition route | Instance 0',\n",
       " 'CEL files',\n",
       " 'Lipoprotein A reportability | Instance 0',\n",
       " 'Cystatin C assay date | Instance 0',\n",
       " 'Mean corpuscular volume acquisition route | Instance 0',\n",
       " 'Spirometer device ID | Instance 0',\n",
       " 'Red blood cell (erythrocyte) count acquisition route | Instance 0',\n",
       " 'Cystatin C correction reason | Instance 0',\n",
       " 'Mean reticulocyte volume acquisition route | Instance 0',\n",
       " 'Haemoglobin concentration acquisition time | Instance 0',\n",
       " 'Potassium in urine device ID | Instance 0',\n",
       " 'Calcium reportability | Instance 0',\n",
       " 'Rheumatoid factor assay date | Instance 0',\n",
       " 'Apolipoprotein A reportability | Instance 0',\n",
       " 'Reticulocyte count acquisition time | Instance 0',\n",
       " 'Mean platelet (thrombocyte) volume device ID | Instance 0',\n",
       " 'C-reactive protein reportability | Instance 0',\n",
       " 'Platelet crit acquisition route | Instance 0',\n",
       " 'Haematocrit percentage acquisition route | Instance 0',\n",
       " 'Reticulocyte percentage acquisition time | Instance 0',\n",
       " 'Calcium correction reason | Instance 0',\n",
       " 'Glucose correction level | Instance 0',\n",
       " 'LDL direct assay date | Instance 0',\n",
       " 'Verbal nterview sign-off timestamp | Instance 0',\n",
       " 'Mean reticulocyte volume device ID | Instance 0',\n",
       " 'Alanine aminotransferase reportability | Instance 0',\n",
       " 'Urate reportability | Instance 0',\n",
       " 'Blood pressure device ID | Instance 0',\n",
       " 'Potassium in urine acquisition time | Instance 0',\n",
       " 'Platelet distribution width acquisition route | Instance 0',\n",
       " 'Data points for blow | Instance 0 | Array 0',\n",
       " 'Nucleated red blood cell count acquisition time | Instance 0',\n",
       " 'Gamma glutamyltransferase reportability | Instance 0',\n",
       " 'Reticulocyte percentage acquisition route | Instance 0',\n",
       " 'Lymphocyte percentage acquisition route | Instance 0',\n",
       " 'Cystatin C correction level | Instance 0',\n",
       " 'Testosterone correction level | Instance 0']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_difference(mci_ad_70, mci_ad_70_clear_drop, '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/01282024/delete_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the clear data\n",
    "ad_death_70_clear_drop.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_merged_clear_drop.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_clear_drop.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_merged_clear_drop.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice: I manually deleted some variables like Time samples collected for 3 samples, so please reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_drop = pd.read_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_merged_clear_drop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge original data and current data (stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this step has stopped, but it still need to be processed\n",
    "1. delete the duplicated BMI\n",
    "2. add Days between AD and Death more conviniently\n",
    "\n",
    "----\n",
    "BMI in ad_death_70_drop is duplicated to 2 columns, so delete one column\n",
    "1. Assessment centre > Physical measures > Anthropometry > Body size measures, participants.p21001\n",
    "2. Assessment centre > Physical measures > Anthropometry > Body composition by impedance, participants.p23104\n",
    "Body mass index (BMI) | Instance 0_y has more missing value, delete this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Body mass index (BMI) | Instance 0_x', 'Body mass index (BMI) | Instance 0_y'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find columns ad_death_70_drop include 'Body mass index (BMI)'\n",
    "ad_death_70_drop.filter(like='Body mass index (BMI)').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Body mass index (BMI) | Instance 0_x', 'Body mass index (BMI) | Instance 0_y'], dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find columns ad_death_70_drop include 'Body mass index (BMI)'\n",
    "mci_ad_70_drop.filter(like='Body mass index (BMI)').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# find which column has more nan values, Body mass index (BMI) | Instance 0_x, Body mass index (BMI) | Instance 0_y\n",
    "print(ad_death_70_drop['Body mass index (BMI) | Instance 0_x'].isna().sum())\n",
    "print(ad_death_70_drop['Body mass index (BMI) | Instance 0_y'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# find which column has more nan values, Body mass index (BMI) | Instance 0_x, Body mass index (BMI) | Instance 0_y\n",
    "print(mci_ad_70_drop['Body mass index (BMI) | Instance 0_x'].isna().sum())\n",
    "print(mci_ad_70_drop['Body mass index (BMI) | Instance 0_y'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Body mass index (BMI) | Instance 0_y and rename Body mass index (BMI) | Instance 0_x to Body mass index (BMI)\n",
    "ad_death_70_drop = ad_death_70_drop.drop(['Body mass index (BMI) | Instance 0_y'], axis=1)\n",
    "ad_death_70_drop = ad_death_70_drop.rename(columns={'Body mass index (BMI) | Instance 0_x': 'Body mass index (BMI) | Instance 0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Body mass index (BMI) | Instance 0_y and rename Body mass index (BMI) | Instance 0_x to Body mass index (BMI)\n",
    "mci_ad_70_drop = mci_ad_70_drop.drop(['Body mass index (BMI) | Instance 0_y'], axis=1)\n",
    "mci_ad_70_drop = mci_ad_70_drop.rename(columns={'Body mass index (BMI) | Instance 0_x': 'Body mass index (BMI) | Instance 0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete 'group' label from ad_death_80_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete 'group' label from ad_death_80_orig\n",
    "ad_death_80_orig = ad_death_80_orig.drop(['group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete 'group' label from mci_ad_80_orig\n",
    "mci_ad_80_orig = mci_ad_80_orig.drop(['group'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. merge ad_death_80_orig and ad_death_70_drop, and find how many difference in this two dataframe: no difference, so we don't need to merge them\n",
    "2. merge mci_ad_80_orig and mci_ad_70_drop, and find how many difference in this two dataframe: no difference, so we don't need to merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ad_death_80_orig and ad_death_70_drop\n",
    "ad_death_80_70 = column_merge(ad_death_70_drop, ad_death_80_orig, 'Participant ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ad_death_80_orig and ad_death_70_drop\n",
    "mci_ad_80_70 = column_merge(mci_ad_70_drop, mci_ad_80_orig, 'Participant ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_80_70.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_80_70_merged.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_80_70.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## column record generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prerequisite:\n",
    "acquire the column record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_80_70_columns = column_record(ad_death_80_70)\n",
    "ad_death_80_70, ad_death_80_70_columns = check_values(ad_death_80_70, ad_death_80_70_columns)\n",
    "ad_death_80_70, ad_death_80_70_columns = column_values(ad_death_80_70, ad_death_80_70_columns)\n",
    "# don't use csv, there are some commas in the ICD10 codes\n",
    "ad_death_80_70_columns = missing_count(ad_death_80_70, ad_death_80_70_columns)\n",
    "# sort ad_death_80_70_columns by availability\n",
    "ad_death_80_70_columns = ad_death_80_70_columns.sort_values(by=['availability'], ascending=False)\n",
    "ad_death_80_70.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_80_70_merged.tsv'), sep='\\t', index=False)\n",
    "ad_death_80_70_columns.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_80_70_merged_column_record.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_80_70_columns = column_record(mci_ad_80_70)\n",
    "mci_ad_80_70, mci_ad_80_70_columns = check_values(mci_ad_80_70, mci_ad_80_70_columns)\n",
    "mci_ad_80_70, mci_ad_80_70_columns = column_values(mci_ad_80_70, mci_ad_80_70_columns)\n",
    "# don't use csv, there are some commas in the ICD10 codes\n",
    "mci_ad_80_70_columns = missing_count(mci_ad_80_70, mci_ad_80_70_columns)\n",
    "# sort mci_ad_80_70_columns by availability\n",
    "mci_ad_80_70_columns = mci_ad_80_70_columns.sort_values(by=['availability'], ascending=False)\n",
    "mci_ad_80_70.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged.tsv', sep='\\t', index=False)\n",
    "mci_ad_80_70_columns.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged_column_record.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature numeric change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* delete 'Date of death | Instance 0', 'Date of alzheimer\\'s disease report', 'Date of all cause dementia report' here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad_death_80_70 = ad_death_80_70.drop(columns=['Date of death | Instance 0', 'Date of alzheimer\\'s disease report', 'Date of all cause dementia report'])\n",
    "ad_death_70_numeric = from_other_to_numeric_all(ad_death_80_70, ad_death_80_70_columns, ['Participant ID', 'Days Between AD and Death'])\n",
    "ad_death_70_numeric.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_numeric.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* delete 'Date of death | Instance 0', 'Date of alzheimer\\'s disease report', 'Date of all cause dementia report', 'Date F00 first reported (dementia in alzheimer\\'s disease)' here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_80_70 = mci_ad_80_70.drop(columns=['Date of alzheimer\\'s disease report', 'Date of all cause dementia report', 'Date F00 first reported (dementia in alzheimer\\'s disease)'])\n",
    "mci_ad_70_numeric = from_other_to_numeric_all(mci_ad_80_70, mci_ad_80_70_columns, ['Participant ID', 'Time Between MCI and AD'])\n",
    "mci_ad_70_numeric.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_numeric.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01282024_1:\n",
    "- 0-1, min_max normalization (!please do variance threshold first)(change only except 0,1)\n",
    "01282024_2:\n",
    "- standard, zscore normalization (!please do variance threshold first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_70_norm = df_minmaxscaler(ad_death_70_numeric.drop(columns=['Participant ID', 'Days Between AD and Death']), ad_death_70_numeric['Days Between AD and Death'])\n",
    "ad_death_70_norm = pd.concat([ad_death_70_numeric[['Participant ID', 'Days Between AD and Death']], ad_death_70_norm], axis=1)\n",
    "ad_death_70_norm.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_norm.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_norm = df_minmaxscaler(mci_ad_70_numeric.drop(columns=['Participant ID', 'Time Between MCI and AD']), mci_ad_70_numeric['Time Between MCI and AD'])\n",
    "mci_ad_70_norm = pd.concat([mci_ad_70_numeric[['Participant ID', 'Time Between MCI and AD']], mci_ad_70_norm], axis=1)\n",
    "mci_ad_70_norm.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_norm.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscore normalization\n",
    "ad_death_70_zscore = df_standard(ad_death_70_numeric.drop(columns=['Participant ID', 'Days Between AD and Death']), ad_death_70_numeric['Days Between AD and Death'])\n",
    "ad_death_70_zscore = pd.concat([ad_death_70_numeric[['Participant ID', 'Days Between AD and Death']], ad_death_70_zscore], axis=1)\n",
    "ad_death_70_zscore.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_zscore.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_zscore = df_standard(mci_ad_70_numeric.drop(columns=['Participant ID', 'Time Between MCI and AD']), mci_ad_70_numeric['Time Between MCI and AD'])\n",
    "mci_ad_70_zscore = pd.concat([mci_ad_70_numeric[['Participant ID', 'Time Between MCI and AD']], mci_ad_70_zscore], axis=1)\n",
    "mci_ad_70_zscore.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_zscore.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer_init is 695\n",
      "buffer_end is 889\n",
      "cutoff_index is 792\n"
     ]
    }
   ],
   "source": [
    "ad_death_70_norm_labeled = label_generate(ad_death_70_norm, 'Days Between AD and Death', 800, buffer=True, buffer_percent=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_norm_labeled = label_generate(mci_ad_70_norm, 'Time Between MCI and AD', 700, buffer=False, buffer_percent=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the missing value\n",
    "01282024_1 availability > 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = ad_death_80_70_columns[ad_death_80_70_columns['availability']>0.67]['name'].tolist()\n",
    "# save only columns in features_selected\n",
    "ad_death_70_selected = column_substring(ad_death_70_norm_labeled, features_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 9824)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_death_70_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 6138)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mci_ad_70_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = mci_ad_80_70_columns[mci_ad_80_70_columns['availability']>0.67]['name'].tolist()\n",
    "# save only columns in features_selected\n",
    "mci_ad_70_selected = column_substring(mci_ad_70_norm, features_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 6130)\n"
     ]
    }
   ],
   "source": [
    "print(mci_ad_70_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use ['Participant ID', 'Date of death | Instance 0', 'Date of alzheimer's disease report', 'Date of all cause dementia report', 'Days Between AD and Death', 'Date F00 first reported (dementia in alzheimer\\'s disease)']\n",
    "\n",
    "label: Days Between AD and Death,Time Between MCI and AD \n",
    "\n",
    "impute the data and then change to numeric\n",
    "\n",
    "then check the unbalanced value, some value with variance threshold too low, delte this columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  by the VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_variance(ad_death_70_numeric.drop(columns=['Participant ID', 'Days Between AD and Death']))\n",
    "features = ['Participant ID', 'Days Between AD and Death'] + features[0]\n",
    "ad_death_70_numeric = ad_death_70_numeric[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_variance(mci_ad_70_numeric.drop(columns=['Participant ID', 'Time Between MCI and AD']))\n",
    "features = ['Participant ID', 'Time Between MCI and AD'] + features[0]\n",
    "mci_ad_70_numeric = mci_ad_70_numeric[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_corr(ad_death_70_numeric.drop(columns=['Participant ID']))\n",
    "features = ['Participant ID'] + features\n",
    "ad_death_70_numeric = ad_death_70_numeric[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_corr(mci_ad_70_numeric.drop(columns=['Participant ID']))\n",
    "features = ['Participant ID'] + features\n",
    "mci_ad_70_numeric = mci_ad_70_numeric[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the chi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ad_death_70_norm.drop(columns=['Participant ID', 'Days Between AD and Death'])\n",
    "target = ad_death_70_norm['Days Between AD and Death']\n",
    "feature_num = len(ad_death_70_norm.columns) - 2\n",
    "features, featurescore, p_value = df_chi2(train, target, feature_num)\n",
    "p_value.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_p_value.tsv'), sep='\\t', index=False)\n",
    "ad_death_70_norm = chi2_filter(ad_death_70_norm, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mci_ad_70_norm.drop(columns=['Participant ID', 'Time Between MCI and AD'])\n",
    "target = mci_ad_70_norm['Time Between MCI and AD']\n",
    "feature_num = len(mci_ad_70_norm.columns) - 2\n",
    "features, featurescore, p_value = df_chi2(train, target, feature_num)\n",
    "p_value.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_p_value.tsv', sep='\\t', index=False)\n",
    "mci_ad_70_norm = chi2_filter(mci_ad_70_norm, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by the ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for anova, first should use the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_fast = ad_death_70_numeric_labeled[ad_death_70_numeric_labeled['label']==1]\n",
    "group_fast = group_fast.drop(columns=['Participant ID', 'Days Between AD and Death', 'label'])\n",
    "group_slow = ad_death_70_numeric_labeled[ad_death_70_numeric_labeled['label']==0]\n",
    "group_slow = group_slow.drop(columns=['Participant ID', 'Days Between AD and Death', 'label'])\n",
    "df_anova_res = df_anova([group_fast, group_slow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anova_cutoff = check_anova(df_anova_res, 0, p_value_cutoff=0.05)\n",
    "df_anova_cutoff.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_anova.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_fast = mci_ad_70_norm_labeled[mci_ad_70_norm_labeled['label']==1]\n",
    "group_fast = group_fast.drop(columns=['Participant ID', 'Time Between MCI and AD', 'label'])\n",
    "group_slow = mci_ad_70_norm_labeled[mci_ad_70_norm_labeled['label']==0]\n",
    "group_slow = group_slow.drop(columns=['Participant ID', 'Time Between MCI and AD', 'label'])\n",
    "df_anova_res = df_anova([group_fast, group_slow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anova_cutoff = check_anova(df_anova_res, 0, p_value_cutoff=0.05)\n",
    "df_anova_cutoff.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_ad_death_70_anova.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by feature selection algorithm: mrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.96it/s]\n"
     ]
    }
   ],
   "source": [
    "features = df_mrmr(ad_death_70_numeric.drop(columns=['Participant ID', 'Days Between AD and Death']), ad_death_70_numeric['Days Between AD and Death'])\n",
    "features = ['Participant ID', 'Days Between AD and Death'] + features\n",
    "ad_death_70_numeric = ad_death_70_numeric[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_mrmr(mci_ad_70_numeric.drop(columns=['Participant ID', 'Time Between MCI and AD']), mci_ad_70_numeric['Time Between MCI and AD'])\n",
    "features = ['Participant ID', 'Time Between MCI and AD'] + features\n",
    "mci_ad_70_numeric = mci_ad_70_numeric[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01082024_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01082024_test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_1 includes the configurations:\n",
    "\n",
    "- data preprocessing:\n",
    "    - 1. delete the keywords list that not works in the experiments, delete duplicated BMI\n",
    "    - 2. the ad_death_70 include all the features in ad_death_80, but add days ad_death\n",
    "    - 3. column record: the data type includes ['numeric', 'categorical', 'date', 'multiple']\n",
    "    - 4. delete 'Date of death | Instance 0', 'Date of alzheimer\\'s disease report', 'Date of all cause dementia report' here.\n",
    "    - 5. feature numeric: change categorical values to one-hot(get_dummies), \n",
    "    change date to age(round to 2 decimals), change multiple value feature to one-hot\n",
    "    - 6. feature normalization: use minmax normalization to scale all the X values distribute range in [0, 1]\n",
    "    - 7. label generation: generate label with cutoff = 800, buffer = 0.2(no buffer, this step decrepated)\n",
    "    - 8. feature selection\n",
    "        - 7.1. availability > 0.67\n",
    "        - 7.2. variance > 0.01\n",
    "        - 7.3. correlation: (don't change), but display the hotplot with corr > 0.5\n",
    "        - 7.4. chi-2: \n",
    "            - 7.4.1 p-value < 0.05\n",
    "            - 7.4.2 not change\n",
    "        - 7.5. ANOVA:\n",
    "            - 7.5.1 anova p-value < 0.05\n",
    "            - 7.5.2 not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* please note ad_death_70_merged_clear.csv is what cleared manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWCASE = 'data/weimin/Data_Dictionary_Showcase.csv'\n",
    "AD_Death_70_clear = 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_merged_clear.csv'\n",
    "AD_Death_80_orig = 'data/weimin/train/ad_death_data_decoded_80.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "showcase = pd.read_csv(os.path.join('..', SHOWCASE))\n",
    "ad_death_70_clear = pd.read_csv(os.path.join('..', AD_Death_70_clear))\n",
    "ad_death_80_orig = pd.read_csv(os.path.join('..', AD_Death_80_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name is held by UKB\n",
      "********************\n",
      "Index(['Volume of EDTA2 plasma held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 red cells held by UKB | Instance 0',\n",
      "       'Volume of EDTA2 buffy held by UKB | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is assay date\n",
      "********************\n",
      "Index(['Vitamin D assay date | Instance 0',\n",
      "       'Total bilirubin assay date | Instance 0',\n",
      "       'Triglycerides assay date | Instance 0',\n",
      "       'Rheumatoid factor assay date | Instance 0',\n",
      "       'Total protein assay date | Instance 0',\n",
      "       'LDL direct assay date | Instance 0',\n",
      "       'Phosphate assay date | Instance 0', 'IGF-1 assay date | Instance 0',\n",
      "       'Urate assay date | Instance 0', 'Testosterone assay date | Instance 0',\n",
      "       'SHBG assay date | Instance 0', 'Lipoprotein A assay date | Instance 0',\n",
      "       'Oestradiol assay date | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is reportability\n",
      "********************\n",
      "Index(['Triglycerides reportability | Instance 0',\n",
      "       'Rheumatoid factor reportability | Instance 0',\n",
      "       'LDL direct reportability | Instance 0',\n",
      "       'Total bilirubin reportability | Instance 0',\n",
      "       'Urate reportability | Instance 0', 'IGF-1 reportability | Instance 0',\n",
      "       'Testosterone reportability | Instance 0',\n",
      "       'Lipoprotein A reportability | Instance 0',\n",
      "       'Vitamin D reportability | Instance 0',\n",
      "       'Oestradiol reportability | Instance 0',\n",
      "       'Total protein reportability | Instance 0',\n",
      "       'Phosphate reportability | Instance 0',\n",
      "       'HDL cholesterol reportability | Instance 0',\n",
      "       'SHBG reportability | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is device ID\n",
      "********************\n",
      "Index(['Blood pressure device ID | Instance 0',\n",
      "       'Height measure device ID | Instance 0',\n",
      "       'Seating box device ID | Instance 0',\n",
      "       'Tape measure device ID | Instance 0',\n",
      "       'Impedance device ID | Instance 0',\n",
      "       'Hand grip dynamometer device ID | Instance 0',\n",
      "       'Spirometer device ID | Instance 0',\n",
      "       'Manual scales device ID | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is acquisition route\n",
      "********************\n",
      "Index(['Haematocrit percentage acquisition route | Instance 0', 'Mean sphered cell volume acquisition route | Instance 0'], dtype='object')\n",
      "column name is acquisition time\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is correction reason\n",
      "********************\n",
      "Index(['Alkaline phosphatase correction reason | Instance 0',\n",
      "       'Albumin correction reason | Instance 0',\n",
      "       'Gamma glutamyltransferase correction reason | Instance 0',\n",
      "       'Cholesterol correction reason | Instance 0',\n",
      "       'Alanine aminotransferase correction reason | Instance 0',\n",
      "       'Urea correction reason | Instance 0',\n",
      "       'Cystatin C correction reason | Instance 0',\n",
      "       'Creatinine correction reason | Instance 0',\n",
      "       'C-reactive protein correction reason | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) correction reason | Instance 0',\n",
      "       'Aspartate aminotransferase correction reason | Instance 0',\n",
      "       'Apolipoprotein B correction reason | Instance 0',\n",
      "       'Calcium correction reason | Instance 0',\n",
      "       'HDL cholesterol correction reason | Instance 0',\n",
      "       'Glucose correction reason | Instance 0',\n",
      "       'Apolipoprotein A correction reason | Instance 0',\n",
      "       'Direct bilirubin correction reason | Instance 0',\n",
      "       'Triglycerides correction reason | Instance 0',\n",
      "       'LDL direct correction reason | Instance 0',\n",
      "       'Total bilirubin correction reason | Instance 0',\n",
      "       'IGF-1 correction reason | Instance 0',\n",
      "       'Urate correction reason | Instance 0',\n",
      "       'Vitamin D correction reason | Instance 0',\n",
      "       'Total protein correction reason | Instance 0',\n",
      "       'Phosphate correction reason | Instance 0',\n",
      "       'SHBG correction reason | Instance 0',\n",
      "       'Testosterone correction reason | Instance 0',\n",
      "       'Lipoprotein A correction reason | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is correction level\n",
      "********************\n",
      "Index(['Alkaline phosphatase correction level | Instance 0',\n",
      "       'Albumin correction level | Instance 0',\n",
      "       'Cholesterol correction level | Instance 0',\n",
      "       'Alanine aminotransferase correction level | Instance 0',\n",
      "       'Urea correction level | Instance 0',\n",
      "       'Creatinine correction level | Instance 0',\n",
      "       'Cystatin C correction level | Instance 0',\n",
      "       'Glycated haemoglobin (HbA1c) correction level | Instance 0',\n",
      "       'Aspartate aminotransferase correction level | Instance 0',\n",
      "       'Apolipoprotein B correction level | Instance 0',\n",
      "       'Calcium correction level | Instance 0',\n",
      "       'HDL cholesterol correction level | Instance 0',\n",
      "       'Glucose correction level | Instance 0',\n",
      "       'Apolipoprotein A correction level | Instance 0',\n",
      "       'Direct bilirubin correction level | Instance 0',\n",
      "       'Triglycerides correction level | Instance 0',\n",
      "       'LDL direct correction level | Instance 0',\n",
      "       'Total bilirubin correction level | Instance 0',\n",
      "       'IGF-1 correction level | Instance 0',\n",
      "       'Urate correction level | Instance 0',\n",
      "       'Total protein correction level | Instance 0',\n",
      "       'Phosphate correction level | Instance 0',\n",
      "       'SHBG correction level | Instance 0',\n",
      "       'Testosterone correction level | Instance 0'],\n",
      "      dtype='object')\n",
      "column name is missing reason\n",
      "********************\n",
      "Index(['Oestradiol missing reason | Instance 0', 'Rheumatoid factor missing reason | Instance 0'], dtype='object')\n",
      "column name is Patient classification on admission (recoded)\n",
      "********************\n",
      "Index(['Patient classification on admission (recoded)'], dtype='object')\n",
      "column name is Diagnoses - ICD10\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Sources of admission to hospital (recoded)\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Inpatient record format\n",
      "********************\n",
      "Index(['Inpatient record format'], dtype='object')\n",
      "column name is authorisation\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is timestamp\n",
      "********************\n",
      "Index(['Program (tactus) version ID (compiler timestamp)'], dtype='object')\n",
      "column name is CEL files\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is HLA imputation values\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Genotype measurement batch\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Genotype measurement plate\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Present in OMOP dataset\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Used in genetic principal components\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Year of survey | Instance 0\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Data points for blow | Instance 0 | Array 0\n",
      "********************\n",
      "Index(['Data points for blow | Instance 0 | Array 0'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ad_death_70_drop = column_delete(ad_death_70_clear, delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Body mass index (BMI) | Instance 0_x', 'Body mass index (BMI) | Instance 0_y'], dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find columns ad_death_70_drop include 'Body mass index (BMI)'\n",
    "ad_death_70_drop.filter(like='Body mass index (BMI)').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# find which column has more nan values, Body mass index (BMI) | Instance 0_x, Body mass index (BMI) | Instance 0_y\n",
    "print(ad_death_70_drop['Body mass index (BMI) | Instance 0_x'].isna().sum())\n",
    "print(ad_death_70_drop['Body mass index (BMI) | Instance 0_y'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Body mass index (BMI) | Instance 0_y and rename Body mass index (BMI) | Instance 0_x to Body mass index (BMI)\n",
    "ad_death_70_drop = ad_death_70_drop.drop(['Body mass index (BMI) | Instance 0_y'], axis=1)\n",
    "ad_death_70_drop = ad_death_70_drop.rename(columns={'Body mass index (BMI) | Instance 0_x': 'Body mass index (BMI) | Instance 0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_80_orig = ad_death_80_orig.drop(['group'], axis=1)\n",
    "ad_death_80_70 = column_merge(ad_death_70_drop, ad_death_80_orig, 'Participant ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_80_70_columns = column_record(ad_death_80_70)\n",
    "ad_death_80_70, ad_death_80_70_columns = check_values(ad_death_80_70, ad_death_80_70_columns)\n",
    "ad_death_80_70, ad_death_80_70_columns = column_values(ad_death_80_70, ad_death_80_70_columns)\n",
    "# don't use csv, there are some commas in the ICD10 codes\n",
    "ad_death_80_70_columns = missing_count(ad_death_80_70, ad_death_80_70_columns)\n",
    "# sort ad_death_80_70_columns by availability\n",
    "ad_death_80_70_columns = ad_death_80_70_columns.sort_values(by=['availability'], ascending=False)\n",
    "ad_death_80_70_columns.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_80_70_merged_column_record.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_80_70 = ad_death_80_70.drop(columns=['Date of death | Instance 0', 'Date of alzheimer\\'s disease report', 'Date of all cause dementia report'])\n",
    "ad_death_70_numeric = from_other_to_numeric_all(ad_death_80_70, ad_death_80_70_columns, ['Participant ID', 'Days Between AD and Death'])\n",
    "ad_death_70_numeric.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_numeric.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_70_norm = df_minmaxscaler(ad_death_70_numeric.drop(columns=['Participant ID', 'Days Between AD and Death']), ad_death_70_numeric['Days Between AD and Death'])\n",
    "ad_death_70_norm = pd.concat([ad_death_70_numeric[['Participant ID', 'Days Between AD and Death']], ad_death_70_norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_70_norm_labeled = label_generate(ad_death_70_norm, 'Days Between AD and Death', 800, buffer=False, buffer_percent=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = ad_death_80_70_columns[ad_death_80_70_columns['availability']>0.67]['name'].tolist()\n",
    "ad_death_70_selected = column_substring(ad_death_70_norm_labeled, features_selected)\n",
    "ad_death_70_selected['label'] = ad_death_70_norm_labeled['label']\n",
    "\n",
    "features = df_variance(ad_death_70_selected.drop(columns=['Participant ID', 'Days Between AD and Death', 'label']))\n",
    "features = ['Participant ID', 'Days Between AD and Death', 'label'] + features[0]\n",
    "ad_death_70_selected = ad_death_70_selected[features]\n",
    "\n",
    "features = df_corr(ad_death_70_selected.drop(columns=['Participant ID', 'label']))\n",
    "features = ['Participant ID', 'label'] + features\n",
    "ad_death_70_selected = ad_death_70_selected[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_death_70_selected.drop(columns=['label']).to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_selected_no_buffer.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. chi-2 p-value<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ad_death_70_selected.drop(columns=['Participant ID', 'Days Between AD and Death', 'label'])\n",
    "target = ad_death_70_selected['Days Between AD and Death']\n",
    "feature_num = len(ad_death_70_selected.columns) - 2\n",
    "features, featurescore, p_value = df_chi2(train, target, feature_num)\n",
    "p_value.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_p_value.tsv'), sep='\\t', index=False)\n",
    "\n",
    "ad_death_70_chi2 = chi2_filter(ad_death_70_selected, p_value)\n",
    "ad_death_70_chi2.drop(columns=['label']).to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_chi2.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. anova p-value < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_fast = ad_death_70_selected[ad_death_70_selected['label']==1]\n",
    "group_fast = group_fast.drop(columns=['Participant ID', 'Days Between AD and Death', 'label'])\n",
    "group_slow = ad_death_70_selected[ad_death_70_selected['label']==0]\n",
    "group_slow = group_slow.drop(columns=['Participant ID', 'Days Between AD and Death', 'label'])\n",
    "df_anova_res = df_anova([group_fast, group_slow])\n",
    "\n",
    "df_anova_cutoff = check_anova(df_anova_res, 0, p_value_cutoff=0.05)\n",
    "df_anova_cutoff.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_anova_res.tsv'), sep='\\t', index=False)\n",
    "\n",
    "ad_death_70_anova = anova_filter(ad_death_70_selected, df_anova_cutoff)\n",
    "ad_death_70_anova.to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_anova.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(979, 477)\n"
     ]
    }
   ],
   "source": [
    "union_features = list(set(ad_death_70_chi2.columns.tolist()).union(set(ad_death_70_anova.columns.tolist())))\n",
    "ad_death_70_union = ad_death_70_selected[union_features]\n",
    "print(ad_death_70_union.shape)\n",
    "ad_death_70_union.drop(columns=['label']).to_csv(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_chunk58_ad_death_70_union_no_buffer.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next part is to produce the field id txt for ukb data retrieve, not related to this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "colummns = ad_death_70_selected.drop(columns=['Days Between AD and Death', 'label']).columns\n",
    "# save colummns to txt\n",
    "column_set_selected = set([col.split('_')[0] for col in colummns])\n",
    "with open(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_death_selected_columns.txt'), 'w') as f:\n",
    "    for item in column_set_selected:\n",
    "        f.write(\"%s\\n\" % item )\n",
    "\n",
    "colummns = ad_death_70_union.drop(columns=['Days Between AD and Death', 'label']).columns\n",
    "# save colummns to txt\n",
    "column_set_union = set([col.split('_')[0] for col in colummns])\n",
    "with open(os.path.join('..', 'data/weimin/01282024/01082024_data_ad_death_union_columns.txt'), 'w') as f:\n",
    "    for item in column_set_union:\n",
    "        f.write(\"%s\\n\" % item )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are some duplicated value in ukb\n",
    "1. BMI\n",
    "2. Vitamin D | Instance 0 (Biological samples / Blood assays / Blood biochemistry, Online follow-up / Diet by 24-hour recall / Estimated nutrients yesterday (obsolete))(keep field id less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create coding_selected with title in column_set\n",
    "field_selected = []\n",
    "for item in column_set_union:\n",
    "    row = coding[coding['title'] == item]\n",
    "    # print(row)\n",
    "    field = row['entity']+'.'+row['name']\n",
    "    if field.shape[0] > 1:\n",
    "        # error\n",
    "        if field.shape[0] == 2:\n",
    "            # keep the smaller one\n",
    "            field_selected.append(field.values[0])\n",
    "        else:\n",
    "            print(item)\n",
    "            print(row.columns)\n",
    "            print(field)\n",
    "            sys.exit()\n",
    "    field_selected.append(field.values[0])\n",
    "field_selected = list(set(field_selected))\n",
    "# save field_selected to txt\n",
    "with open(os.path.join('..', 'data/weimin/field/01282024/01082024_data_ad_death_union_fields.txt'), 'w') as f:\n",
    "    for item in field_selected:\n",
    "        f.write(\"%s\\n\" % item )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02222024_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02222024_test_1: make the mci_ad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_1 includes the configurations:\n",
    "\n",
    "- data preprocessing:\n",
    "    - 1. delete the keywords list that not works in the experiments, delete duplicated BMI\n",
    "    - 2. the mci_ad_70 include all the features in mci_ad_80, but add days mci_ad\n",
    "    - 3. column record: the data type includes ['numeric', 'categorical', 'date', 'multiple']\n",
    "    - 4. delete 'Date of death | Instance 0', 'Date of alzheimer\\'s disease report', 'Date of all cause dementia report', 'Date F00 first reported (dementia in alzheimer\\'s disease)' here.\n",
    "    - 5. feature numeric: change categorical values to one-hot(get_dummies), \n",
    "    change date to age(round to 2 decimals), change multiple value feature to one-hot\n",
    "    - 6. feature normalization: use minmax normalization to scale all the X values distribute range in [0, 1] (minmax)\n",
    "    - 7. label generation: generate label with cutoff = 700, buffer = 0.2(no buffer, this step decrepated, but must do because of ANOVA)\n",
    "    - 8. feature selection\n",
    "        - 7.1. availability > 0.67\n",
    "        - 7.2. variance > 0.01\n",
    "        - 7.3. correlation: (don't change), but display the hotplot with corr > 0.5\n",
    "        - 7.4. chi-2: \n",
    "            - 7.4.1 p-value < 0.05\n",
    "            - 7.4.2 not change\n",
    "        - 7.5. ANOVA:\n",
    "            - 7.5.1 anova p-value < 0.05\n",
    "            - 7.5.2 not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWCASE = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/Data_Dictionary_Showcase.csv'\n",
    "MCI_AD_70_clear = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_merged_clear_drop.csv'\n",
    "MCI_AD_80_orig = '/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/train/mci_ad_data_decoded_80.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "showcase = pd.read_csv(SHOWCASE)\n",
    "mci_ad_70_clear = pd.read_csv(MCI_AD_70_clear)\n",
    "mci_ad_80_orig = pd.read_csv(MCI_AD_80_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* please note because we load the clear and drop data, so we don't need to do these part, only check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name is held by UKB\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is assay date\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is reportability\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is device ID\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is acquisition route\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is acquisition time\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is correction reason\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is correction level\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is missing reason\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Patient classification on admission (recoded)\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Diagnoses - ICD10\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Sources of admission to hospital (recoded)\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Inpatient record format\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is authorisation\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is timestamp\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is CEL files\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is HLA imputation values\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Genotype measurement batch\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Genotype measurement plate\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Present in OMOP dataset\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Used in genetic principal components\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Year of survey | Instance 0\n",
      "********************\n",
      "Index([], dtype='object')\n",
      "column name is Data points for blow | Instance 0 | Array 0\n",
      "********************\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "mci_ad_70_drop = column_delete(mci_ad_70_clear, delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Body mass index (BMI) | Instance 0'], dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find columns ad_death_70_drop include 'Body mass index (BMI)'\n",
    "mci_ad_70_drop.filter(like='Body mass index (BMI)').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# find which column has more nan values, Body mass index (BMI) | Instance 0_x, Body mass index (BMI) | Instance 0_y\n",
    "print(mci_ad_70_drop['Body mass index (BMI) | Instance 0_x'].isna().sum())\n",
    "print(mci_ad_70_drop['Body mass index (BMI) | Instance 0_y'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Body mass index (BMI) | Instance 0_y and rename Body mass index (BMI) | Instance 0_x to Body mass index (BMI)\n",
    "mci_ad_70_drop = mci_ad_70_drop.drop(['Body mass index (BMI) | Instance 0_y'], axis=1)\n",
    "mci_ad_70_drop = mci_ad_70_drop.rename(columns={'Body mass index (BMI) | Instance 0_x': 'Body mass index (BMI) | Instance 0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_80_orig = mci_ad_80_orig.drop(['group'], axis=1)\n",
    "mci_ad_80_70 = column_merge(mci_ad_70_drop, mci_ad_80_orig, 'Participant ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_80_70_columns = column_record(mci_ad_80_70)\n",
    "mci_ad_80_70, mci_ad_80_70_columns = check_values(mci_ad_80_70, mci_ad_80_70_columns)\n",
    "mci_ad_80_70, mci_ad_80_70_columns = column_values(mci_ad_80_70, mci_ad_80_70_columns)\n",
    "# don't use csv, there are some commas in the ICD10 codes\n",
    "mci_ad_80_70_columns = missing_count(mci_ad_80_70, mci_ad_80_70_columns)\n",
    "# sort mci_ad_80_70_columns by availability\n",
    "mci_ad_80_70_columns = mci_ad_80_70_columns.sort_values(by=['availability'], ascending=False)\n",
    "mci_ad_80_70.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged.tsv', sep='\\t', index=False)\n",
    "mci_ad_80_70_columns.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_80_70_merged_column_record.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_80_70 = mci_ad_80_70.drop(columns=['Date of alzheimer\\'s disease report', 'Date of all cause dementia report', 'Date F00 first reported (dementia in alzheimer\\'s disease)'])\n",
    "mci_ad_70_numeric = from_other_to_numeric_all(mci_ad_80_70, mci_ad_80_70_columns, ['Participant ID', 'Time Between MCI and AD'])\n",
    "mci_ad_70_numeric.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_numeric.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_norm = df_minmaxscaler(mci_ad_70_numeric.drop(columns=['Participant ID', 'Time Between MCI and AD']), mci_ad_70_numeric['Time Between MCI and AD'])\n",
    "mci_ad_70_norm = pd.concat([mci_ad_70_numeric[['Participant ID', 'Time Between MCI and AD']], mci_ad_70_norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_norm_labeled = label_generate(mci_ad_70_norm, 'Time Between MCI and AD', 700, buffer=False, buffer_percent=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 6131)\n",
      "(177, 3341)\n",
      "(177, 3341)\n"
     ]
    }
   ],
   "source": [
    "features_selected = mci_ad_80_70_columns[mci_ad_80_70_columns['availability']>0.67]['name'].tolist()\n",
    "mci_ad_70_selected = column_substring(mci_ad_70_norm, features_selected)\n",
    "mci_ad_70_selected['label'] = mci_ad_70_norm_labeled['label']\n",
    "print(mci_ad_70_selected.shape)\n",
    "\n",
    "features = df_variance(mci_ad_70_selected.drop(columns=['Participant ID', 'Time Between MCI and AD']))\n",
    "features = ['Participant ID', 'Time Between MCI and AD'] + features[0]\n",
    "mci_ad_70_selected = mci_ad_70_selected[features]\n",
    "print(mci_ad_70_selected.shape)\n",
    "\n",
    "features = df_corr(mci_ad_70_selected.drop(columns=['Participant ID', 'label']))\n",
    "features = ['Participant ID', 'label'] + features\n",
    "mci_ad_70_selected = mci_ad_70_selected[features]\n",
    "print(mci_ad_70_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_ad_70_selected.drop(columns=['label']).to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_selected_no_buffer.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. chi-2 p-value<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mci_ad_70_selected.drop(columns=['Participant ID', 'Time Between MCI and AD', 'label'])\n",
    "target = mci_ad_70_selected['Time Between MCI and AD']\n",
    "feature_num = len(mci_ad_70_selected.columns) - 2\n",
    "features, featurescore, p_value = df_chi2(train, target, feature_num)\n",
    "p_value.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_p_value.tsv', sep='\\t', index=False)\n",
    "\n",
    "mci_ad_70_chi2 = chi2_filter(mci_ad_70_selected, p_value)\n",
    "mci_ad_70_chi2.drop(columns=['label']).to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_chi2.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. anova p-value < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_fast = mci_ad_70_selected[mci_ad_70_selected['label']==1]\n",
    "group_fast = group_fast.drop(columns=['Participant ID', 'Time Between MCI and AD', 'label'])\n",
    "group_slow = mci_ad_70_selected[mci_ad_70_selected['label']==0]\n",
    "group_slow = group_slow.drop(columns=['Participant ID', 'Time Between MCI and AD', 'label'])\n",
    "df_anova_res = df_anova([group_fast, group_slow])\n",
    "\n",
    "df_anova_cutoff = check_anova(df_anova_res, 0, p_value_cutoff=0.05)\n",
    "df_anova_cutoff.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_anova_res.tsv', sep='\\t', index=False)\n",
    "\n",
    "mci_ad_70_anova = anova_filter(mci_ad_70_selected, df_anova_cutoff)\n",
    "mci_ad_70_anova.to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_anova.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 170)\n"
     ]
    }
   ],
   "source": [
    "union_features = list(set(mci_ad_70_chi2.columns.tolist()).union(set(mci_ad_70_anova.columns.tolist())))\n",
    "mci_ad_70_union = mci_ad_70_selected[union_features]\n",
    "print(mci_ad_70_union.shape)\n",
    "mci_ad_70_union.drop(columns=['label']).to_csv('/blue/yonghui.wu/weimin.meng/AD_Progression/data/weimin/02222024/02222024_data_ad_chunk58_mci_ad_70_union_no_buffer.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
